{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpjxLG9DpP78",
        "outputId": "b738e449-eb08-465e-cf1f-13ebbe4f1473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8l7WycPlvMC"
      },
      "source": [
        "# 1政策文本嵌入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvteNDMWmBgL",
        "outputId": "e5827b1a-c6a2-4cf8-94cd-d597bf00ae13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing API client...\n",
            "API client initialized successfully.\n",
            "Successfully loaded 13532 total records from /content/drive/MyDrive/Colab Notebooks/验证3+5/gemini_merged_policy_data_with_labels_v2.json\n",
            "Filtering policies and preparing text for embedding...\n",
            "Found 13532 valid policies to process.\n",
            "Generating embeddings via API using model: qwen3-embedding:8b. This may take a while...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "API Call Progress: 100%|██████████| 136/136 [07:46<00:00,  3.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving data with embeddings to /content/drive/MyDrive/Colab Notebooks/验证3+5/policies_with_embeddings_api.json...\n",
            "✅ Success!\n",
            "Processed 13532 policies and saved to /content/drive/MyDrive/Colab Notebooks/验证3+5/policies_with_embeddings_api.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- API 配置 ---\n",
        "# 请将此处的 Token 替换为您自己的 Token\n",
        "# 为安全起见，建议使用环境变量等方式管理 Token，此处为演示方便直接写入。\n",
        "API_TOKEN = \"\"\n",
        "BASE_URL = \"https://uni-api.cstcloud.cn/v1\"\n",
        "EMBEDDING_MODEL = \"qwen3-embedding:8b\"\n",
        "\n",
        "def generate_embeddings_with_api(input_file_path, output_file_path):\n",
        "    \"\"\"\n",
        "    加载政策数据，过滤无效条目，通过调用API生成文本嵌入，并将结果保存到新的JSON文件。\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): 输入的JSON文件路径。\n",
        "        output_file_path (str): 输出的JSON文件路径。\n",
        "    \"\"\"\n",
        "    # 1. 初始化 OpenAI API 客户端\n",
        "    # 使用您指定的 BASE_URL 和 API_TOKEN\n",
        "    print(\"Initializing API client...\")\n",
        "    try:\n",
        "        client = OpenAI(\n",
        "            api_key=API_TOKEN,\n",
        "            base_url=BASE_URL,\n",
        "        )\n",
        "        print(\"API client initialized successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing API client: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. 读取原始JSON数据\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            all_policies = json.load(f)\n",
        "        print(f\"Successfully loaded {len(all_policies)} total records from {input_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at {input_file_path}\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {input_file_path}\")\n",
        "        return\n",
        "\n",
        "    # 3. 过滤数据并准备用于嵌入的文本\n",
        "    valid_policies = []\n",
        "    texts_to_embed = []\n",
        "    print(\"Filtering policies and preparing text for embedding...\")\n",
        "    for policy in all_policies:\n",
        "        short_desc = policy.get('ShortDescription')\n",
        "        # 过滤机制：确保ShortDescription不为空或仅包含空白字符\n",
        "        if short_desc and str(short_desc).strip():\n",
        "            # 将原始名称和简短描述合并为一个文本字符串\n",
        "            # 如果NameEnglish为空，则使用空字符串代替\n",
        "            name_orig = policy.get('NameEnglish', '') or ''\n",
        "            combined_text = f\"{name_orig}. {short_desc}\"\n",
        "\n",
        "            texts_to_embed.append(combined_text)\n",
        "            valid_policies.append(policy)\n",
        "\n",
        "    print(f\"Found {len(valid_policies)} valid policies to process.\")\n",
        "    if not valid_policies:\n",
        "        print(\"No valid policies to process. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 4. 通过API分批生成嵌入向量\n",
        "    print(f\"Generating embeddings via API using model: {EMBEDDING_MODEL}. This may take a while...\")\n",
        "\n",
        "    all_embeddings = []\n",
        "    batch_size = 100  # 设置合理的批处理大小以避免单次请求数据过多\n",
        "\n",
        "    for i in tqdm(range(0, len(texts_to_embed), batch_size), desc=\"API Call Progress\"):\n",
        "        batch_texts = texts_to_embed[i:i + batch_size]\n",
        "\n",
        "        try:\n",
        "            # 调用 embedding API\n",
        "            response = client.embeddings.create(\n",
        "                model=EMBEDDING_MODEL,\n",
        "                input=batch_texts\n",
        "            )\n",
        "            # 从返回结果中提取 embedding 向量\n",
        "            batch_embeddings = [item.embedding for item in response.data]\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during API call for batch starting at index {i}: {e}\")\n",
        "            print(\"Stopping the process. Please check your API key, network connection, or API service status.\")\n",
        "            return # 遇到错误时终止程序\n",
        "\n",
        "    # 5. 将嵌入向量添加回字典列表\n",
        "    # 此时 all_embeddings 的顺序与 valid_policies 完全对应\n",
        "    for i, policy in enumerate(valid_policies):\n",
        "        policy['embed'] = all_embeddings[i]\n",
        "\n",
        "    # 6. 保存带有嵌入向量的新JSON文件\n",
        "    print(f\"\\nSaving data with embeddings to {output_file_path}...\")\n",
        "    try:\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(valid_policies, f, ensure_ascii=False, indent=4)\n",
        "        print(\"✅ Success!\")\n",
        "        print(f\"Processed {len(valid_policies)} policies and saved to {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "\n",
        "# --- 主程序入口 ---\n",
        "if __name__ == '__main__':\n",
        "    # 请将此路径替换为您自己的文件路径\n",
        "    # 注意：在Google Colab中运行时，请确保文件路径正确无误\n",
        "    INPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/验证3+5/gemini_merged_policy_data_with_labels_v2.json'\n",
        "    OUTPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/验证3+5/policies_with_embeddings_api.json'\n",
        "\n",
        "    generate_embeddings_with_api(INPUT_JSON_PATH, OUTPUT_JSON_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KviyNCpVXfOq"
      },
      "source": [
        "# 2无监督分类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w7v2fh0Fk83"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Unsupervised Classification: BERTopic and Hierarchical Topic Modeling with HTM-WS (Improved)\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "required_packages = [\n",
        "    \"bertopic\",\n",
        "    \"hdbscan\",\n",
        "    \"umap-learn\",\n",
        "    \"nltk\",\n",
        "    \"tqdm\",\n",
        "    \"openai\",  # Added for API access\n",
        "    \"tenacity\"  # Added for retry functionality\n",
        "]\n",
        "\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        print(f\"Installing {package}...\")\n",
        "        install(package)\n",
        "    except Exception as e:\n",
        "        print(f\"Error installing {package}: {e}\")\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import hdbscan\n",
        "import umap\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import warnings\n",
        "from openai import OpenAI\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "class Qwen3EmbeddingModel:\n",
        "    \"\"\"\n",
        "    A real-time embedding model using Qwen3-8b API.\n",
        "    This properly embeds documents and keywords using the Qwen3 API.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the Qwen3 embedding model.\n",
        "        \"\"\"\n",
        "        self.client = OpenAI(\n",
        "            api_key=\"f71888795322d9ab77a5508cee879f70a301f1f72de3aaf520858e16ae645d72\",\n",
        "            base_url=\"https://uni-api.cstcloud.cn/v1\"\n",
        "        )\n",
        "        self.model = \"qwen3-embedding:8b\"\n",
        "        self.embedding_dim = 4096  # Qwen3-8b's embedding dimension\n",
        "        print(f\"Initialized Qwen3 embedding model with dimension: {self.embedding_dim}\")\n",
        "\n",
        "    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=4, max=60))\n",
        "    def embed(self, documents, verbose=False):\n",
        "        \"\"\"\n",
        "        Embed documents using Qwen3-8b API.\n",
        "\n",
        "        Args:\n",
        "            documents (list): List of documents to embed\n",
        "            verbose (bool): Whether to print progress\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of embeddings with shape (len(documents), embedding_dim)\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f\"Embedding {len(documents)} texts with Qwen3 API\")\n",
        "\n",
        "        # Process in batches to avoid API limits\n",
        "        batch_size = 100\n",
        "        all_embeddings = []\n",
        "\n",
        "        for i in tqdm(range(0, len(documents), batch_size), disable=not verbose):\n",
        "            batch = documents[i:i+batch_size]\n",
        "            try:\n",
        "                response = self.client.embeddings.create(\n",
        "                    input=batch,\n",
        "                    model=self.model\n",
        "                )\n",
        "                # Extract embeddings from the response\n",
        "                batch_embeddings = [item.embedding for item in response.data]\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "                # Respect rate limits\n",
        "                time.sleep(0.5)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during embedding batch {i}-{i+batch_size}: {e}\")\n",
        "                # Return a fallback embedding for this batch\n",
        "                fallback_emb = [[0.0] * self.embedding_dim] * len(batch)\n",
        "                all_embeddings.extend(fallback_emb)\n",
        "\n",
        "        return np.array(all_embeddings)\n",
        "\n",
        "class BERTopicAnalyzer:\n",
        "    def __init__(self, input_path, output_dir, use_api_for_embeddings=True):\n",
        "        \"\"\"\n",
        "        Initialize the BERTopicAnalyzer with paths and parameters.\n",
        "        Args:\n",
        "            input_path (str): Path to the JSON file with embedded policy data.\n",
        "            output_dir (str): Directory to save results.\n",
        "            use_api_for_embeddings (bool): Whether to use Qwen3 API for embeddings.\n",
        "        \"\"\"\n",
        "        self.input_path = input_path\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Flag to decide whether to use the API for embeddings\n",
        "        self.use_api_for_embeddings = use_api_for_embeddings\n",
        "\n",
        "        # --- Model Parameters ---\n",
        "        self.n_neighbors = 15       # UMAP: balances local vs. global structure\n",
        "        self.min_cluster_size = 30  # HDBSCAN: minimum size for a topic cluster\n",
        "        self.random_state = 42      # For reproducibility of UMAP\n",
        "\n",
        "        # --- Data & Stopwords ---\n",
        "        self.documents = []\n",
        "        self.embeddings = np.array([])\n",
        "        self.doc_ids = []\n",
        "\n",
        "        # Initialize the embedding model\n",
        "        if self.use_api_for_embeddings:\n",
        "            self.embedding_model = Qwen3EmbeddingModel()\n",
        "        else:\n",
        "            self.embedding_model = None\n",
        "\n",
        "        self.load_data()\n",
        "        self.setup_stopwords()\n",
        "\n",
        "        # --- Models ---\n",
        "        self.topic_model = None\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load policy data with embeddings from the JSON file.\"\"\"\n",
        "        print(f\"Loading data from {self.input_path}...\")\n",
        "        with open(self.input_path, 'r', encoding='utf-8') as f:\n",
        "            policies = json.load(f)\n",
        "\n",
        "        temp_docs, temp_embeds, temp_ids = [], [], []\n",
        "        for i, policy in enumerate(policies):\n",
        "            name = policy.get('NameEnglish', '') or ''\n",
        "            desc = policy.get('ShortDescription', '') or ''\n",
        "            combined_text = f\"{name}. {desc}\".strip()\n",
        "\n",
        "            # Only add documents with sufficient content\n",
        "            if len(combined_text) > 10:\n",
        "                temp_docs.append(combined_text)\n",
        "                # If we're using API, we'll compute embeddings later\n",
        "                if not self.use_api_for_embeddings and 'embed' in policy:\n",
        "                    temp_embeds.append(policy['embed'])\n",
        "                temp_ids.append(i) # Store original index as doc_id\n",
        "\n",
        "        self.documents = temp_docs\n",
        "        self.doc_ids = temp_ids\n",
        "\n",
        "        # If we're using the API, compute embeddings now\n",
        "        if self.use_api_for_embeddings:\n",
        "            print(\"Computing embeddings using Qwen3 API (this may take some time)...\")\n",
        "            self.embeddings = self.embedding_model.embed(self.documents, verbose=True)\n",
        "        else:\n",
        "            self.embeddings = np.array(temp_embeds)\n",
        "\n",
        "        print(f\"Loaded {len(self.documents)} documents with embeddings.\")\n",
        "\n",
        "    def setup_stopwords(self):\n",
        "        \"\"\"Download and configure stopwords.\"\"\"\n",
        "        try:\n",
        "            base_stopwords = stopwords.words('english')\n",
        "        except LookupError:\n",
        "            print(\"Downloading NLTK stopwords...\")\n",
        "            nltk.download('stopwords')\n",
        "            base_stopwords = stopwords.words('english')\n",
        "\n",
        "        domain_stopwords = [\n",
        "            'policy', 'measure', 'action', 'law', 'government',\n",
        "            'regulation', 'support', 'development', 'research', 'innovation',\n",
        "            'technology', 'science',\n",
        "            'para', 'use', 'based', 'provide', 'new', 'including', 'public',\n",
        "            'sector', 'countries', 'value', 'mainstreaming', 'access', 'activities',\n",
        "            'capacities', 'change', 'opportunities', 'quality', 'level',\n",
        "            'smes', 'sme', 'ist', 'national', 'international', 'federal',\n",
        "            'regional', 'european'\n",
        "        ]\n",
        "        self.stopwords = base_stopwords + domain_stopwords\n",
        "\n",
        "    def run_flat_topic_modeling(self):\n",
        "        \"\"\"Phase 1: Run BERTopic for flat topic discovery.\"\"\"\n",
        "        print(\"\\n--- Phase 1: Running Flat Topic Modeling with BERTopic ---\")\n",
        "\n",
        "        # Configure UMAP for dimensionality reduction\n",
        "        umap_model = umap.UMAP(n_neighbors=self.n_neighbors, n_components=5,\n",
        "                               min_dist=0.0, metric='cosine', random_state=self.random_state)\n",
        "\n",
        "        # Configure HDBSCAN for clustering\n",
        "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=self.min_cluster_size,\n",
        "                                        metric='euclidean', cluster_selection_method='eom',\n",
        "                                        prediction_data=True)\n",
        "\n",
        "        # Configure CountVectorizer with our custom stopwords\n",
        "        vectorizer = CountVectorizer(stop_words=self.stopwords, min_df=5, max_df=0.8)\n",
        "\n",
        "        # Configure advanced representation models for better topic interpretability\n",
        "        keybert_model = KeyBERTInspired()\n",
        "        mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "        representation_model = {\n",
        "            \"KeyBERT\": keybert_model,\n",
        "            \"MMR\": mmr_model\n",
        "        }\n",
        "\n",
        "        # Initialize and fit BERTopic - now with the real-time embedding model\n",
        "        self.topic_model = BERTopic(\n",
        "            # If using API, provide the model; otherwise, pass embeddings directly\n",
        "            embedding_model=self.embedding_model if self.use_api_for_embeddings else None,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer,\n",
        "            representation_model=representation_model,\n",
        "            verbose=True,\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=True\n",
        "        )\n",
        "\n",
        "        print(\"Fitting BERTopic model...\")\n",
        "        if self.use_api_for_embeddings:\n",
        "            # If using API, let the model embed everything\n",
        "            topics, _ = self.topic_model.fit_transform(self.documents)\n",
        "        else:\n",
        "            # Otherwise pass pre-computed embeddings\n",
        "            topics, _ = self.topic_model.fit_transform(self.documents, self.embeddings)\n",
        "\n",
        "        print(\"\\nPhase 1 complete. Saving results and visualizations...\")\n",
        "        self.save_topic_info()\n",
        "        self.generate_visualizations()\n",
        "        print(\"\\nPlease analyze 'topic_info.csv' and 'topic_keywords.json' to perform manual mapping for Phase 2.\")\n",
        "        return self.topic_model\n",
        "\n",
        "    def run_hierarchical_topic_modeling(self):\n",
        "        \"\"\"\n",
        "        Phase 2: Implement HTM-WS by running nested BERTopic models on\n",
        "        theoretically-grouped synthetic sub-corpora.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Phase 2: Running Hierarchical Topic Modeling (HTM-WS) ---\")\n",
        "\n",
        "        if self.topic_model is None:\n",
        "            print(\"Error: Flat topic model (Phase 1) must be run first.\")\n",
        "            return\n",
        "\n",
        "        # ========================== CRITICAL MANUAL STEP ==========================\n",
        "        # After running Phase 1, you MUST manually inspect the generated topics\n",
        "        # (e.g., from 'topic_info.csv') and map them to your theoretical framework.\n",
        "        # This dictionary is the bridge between empirical findings and your theory.\n",
        "        #\n",
        "        # EXAMPLE:\n",
        "        # manual_topic_mapping = {\n",
        "        #     \"macro\": [0, 5, 12, 18],  # Topics that correspond to macro-level policies\n",
        "        #     \"micro\": [1, 2, 3, 4, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17] # Topics for micro-level\n",
        "        # }\n",
        "        #\n",
        "        # FOR THIS RUN, PLEASE REPLACE THIS EXAMPLE MAPPING WITH YOUR ACTUAL ANALYSIS.\n",
        "        # ==========================================================================\n",
        "        manual_topic_mapping = {\n",
        "            \"macro\": [0, 1, 2],  # Placeholder: Replace with your actual topic IDs\n",
        "            \"micro\": [3, 4, 5, 6, 7]   # Placeholder: Replace with your actual topic IDs\n",
        "        }\n",
        "        print(f\"Using manual mapping for HTM-WS: {manual_topic_mapping}\")\n",
        "\n",
        "        # Get the document-topic assignments from the flat model\n",
        "        doc_info = self.topic_model.get_document_info(self.documents)\n",
        "        # Map original document indices (doc_ids) to new DataFrame indices\n",
        "        doc_info['Original_Index'] = self.doc_ids\n",
        "        doc_info = doc_info.set_index('Original_Index')\n",
        "\n",
        "        hierarchical_results = {}\n",
        "\n",
        "        # Process each theoretical category (macro and micro)\n",
        "        for category, topic_ids in manual_topic_mapping.items():\n",
        "            print(f\"\\nProcessing category: '{category.upper()}'\")\n",
        "\n",
        "            # --- 1. Create Synthetic Sub-Corpus ---\n",
        "            # Get documents and embeddings belonging to this category\n",
        "            category_doc_indices = doc_info[doc_info.Topic.isin(topic_ids)].index.tolist()\n",
        "\n",
        "            if len(category_doc_indices) < self.min_cluster_size:\n",
        "                print(f\"  Skipping '{category}': only {len(category_doc_indices)} documents found (less than min_cluster_size).\")\n",
        "                continue\n",
        "\n",
        "            # We need to map original indices back to their positions in the self.documents list\n",
        "            corpus_indices = [self.doc_ids.index(orig_idx) for orig_idx in category_doc_indices]\n",
        "            sub_corpus_docs = [self.documents[i] for i in corpus_indices]\n",
        "\n",
        "            if not self.use_api_for_embeddings:\n",
        "                sub_corpus_embeddings = self.embeddings[corpus_indices]\n",
        "\n",
        "            print(f\"  Created sub-corpus for '{category}' with {len(sub_corpus_docs)} documents.\")\n",
        "\n",
        "            # --- 2. Run Nested BERTopic Model ---\n",
        "            print(f\"  Running nested BERTopic on '{category}' sub-corpus...\")\n",
        "            sub_topic_model = self.create_subtopic_model()\n",
        "\n",
        "            if self.use_api_for_embeddings:\n",
        "                # Let the model compute embeddings using the API\n",
        "                sub_topics, _ = sub_topic_model.fit_transform(sub_corpus_docs)\n",
        "            else:\n",
        "                # Use pre-computed embeddings\n",
        "                sub_topics, _ = sub_topic_model.fit_transform(sub_corpus_docs, sub_corpus_embeddings)\n",
        "\n",
        "            # --- 3. Save and Visualize Results ---\n",
        "            sub_topic_info = sub_topic_model.get_topic_info()\n",
        "            sub_topic_info.to_csv(f\"{self.output_dir}/htm_ws_{category}_subtopic_info.csv\", index=False)\n",
        "\n",
        "            # Save keywords\n",
        "            sub_topic_keywords = {}\n",
        "            for sub_topic_id in sub_topic_info['Topic']:\n",
        "                if sub_topic_id != -1:\n",
        "                    keywords = sub_topic_model.get_topic(sub_topic_id)\n",
        "                    sub_topic_keywords[f\"SubTopic_{sub_topic_id}\"] = [kw[0] for kw in keywords]\n",
        "\n",
        "            hierarchical_results[category] = {\n",
        "                \"parent_topics\": topic_ids,\n",
        "                \"num_documents\": len(sub_corpus_docs),\n",
        "                \"num_sub_topics_found\": len(sub_topic_info[sub_topic_info.Topic != -1]),\n",
        "                \"sub_topics\": sub_topic_keywords\n",
        "            }\n",
        "\n",
        "            # Visualize\n",
        "            try:\n",
        "                fig = sub_topic_model.visualize_topics()\n",
        "                fig.write_html(f\"{self.output_dir}/htm_ws_{category}_subtopics_visualization.html\")\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"  Could not generate topic visualization for {category}: {e}\")\n",
        "\n",
        "        # Save the overall hierarchical structure summary\n",
        "        with open(f\"{self.output_dir}/hierarchical_structure_summary.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(hierarchical_results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(\"\\nPhase 2 complete. Hierarchical analysis results saved.\")\n",
        "        return hierarchical_results\n",
        "\n",
        "    def create_subtopic_model(self):\n",
        "        \"\"\"Create a new BERTopic instance with parameters suitable for smaller sub-corpora.\"\"\"\n",
        "        umap_model = umap.UMAP(n_neighbors=10, n_components=5, min_dist=0.0,\n",
        "                              metric='cosine', random_state=self.random_state)\n",
        "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=15, metric='euclidean',\n",
        "                                       cluster_selection_method='eom', prediction_data=True)\n",
        "        vectorizer = CountVectorizer(stop_words=self.stopwords, min_df=3, max_df=0.85)\n",
        "\n",
        "        return BERTopic(\n",
        "            # Use the same embedding approach as the main model\n",
        "            embedding_model=self.embedding_model if self.use_api_for_embeddings else None,\n",
        "            umap_model=umap_model,\n",
        "            hdbscan_model=hdbscan_model,\n",
        "            vectorizer_model=vectorizer,\n",
        "            verbose=False,\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=False\n",
        "        )\n",
        "\n",
        "    def save_topic_info(self):\n",
        "        \"\"\"Save topic information from the flat model to files.\"\"\"\n",
        "        topic_info = self.topic_model.get_topic_info()\n",
        "        topic_info.to_csv(f\"{self.output_dir}/topic_info.csv\", index=False)\n",
        "\n",
        "        topic_keywords = {}\n",
        "        for topic_id in topic_info['Topic']:\n",
        "            if topic_id != -1:\n",
        "                keywords = self.topic_model.get_topic(topic_id)\n",
        "                topic_keywords[str(topic_id)] = [kw[0] for kw in keywords]\n",
        "\n",
        "        with open(f\"{self.output_dir}/topic_keywords.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(topic_keywords, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Topic information saved to {self.output_dir}\")\n",
        "\n",
        "    def generate_visualizations(self):\n",
        "        \"\"\"Generate and save visualizations for the flat model.\"\"\"\n",
        "        print(\"Generating visualizations for the flat model...\")\n",
        "        try:\n",
        "            # 1. Interactive 2D projection\n",
        "            fig = self.topic_model.visualize_topics()\n",
        "            fig.write_html(f\"{self.output_dir}/topics_visualization.html\")\n",
        "\n",
        "            # 2. Hierarchical clustering dendrogram\n",
        "            fig = self.topic_model.visualize_hierarchy()\n",
        "            fig.write_html(f\"{self.output_dir}/topic_hierarchy.html\")\n",
        "\n",
        "            # 3. Bar charts for top N topics\n",
        "            fig = self.topic_model.visualize_barchart(top_n_topics=12)\n",
        "            fig.write_html(f\"{self.output_dir}/topic_barchart.html\")\n",
        "\n",
        "            # 4. Topic similarity matrix\n",
        "            fig = self.topic_model.visualize_heatmap(top_n_topics=20)\n",
        "            fig.write_html(f\"{self.output_dir}/topic_similarity_heatmap.html\")\n",
        "\n",
        "            print(f\"Interactive visualizations saved as HTML files in {self.output_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during visualization generation: {e}\")\n",
        "            print(\"Please check if the model produced enough topics.\")\n",
        "        plt.close('all') # Close all plot figures\n",
        "\n",
        "    def run_full_analysis(self):\n",
        "        \"\"\"Execute the complete unsupervised analysis pipeline.\"\"\"\n",
        "        # Phase 1: Flat topic modeling\n",
        "        self.run_flat_topic_modeling()\n",
        "\n",
        "        # Phase 2: Hierarchical topic modeling\n",
        "        self.run_hierarchical_topic_modeling()\n",
        "\n",
        "        print(\"\\nUnsupervised analysis complete!\")\n",
        "        print(f\"All results saved to {self.output_dir}\")\n",
        "        return self.topic_model\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure you are running this in an environment with sufficient memory.\n",
        "    # Google Colab Pro is recommended for datasets of this size.\n",
        "    INPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/验证3+5/policies_with_embeddings_api.json'\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/验证3+5/bertopic_results_improved'\n",
        "\n",
        "    # Set use_api_for_embeddings=True to use Qwen3 API for all embeddings\n",
        "    analyzer = BERTopicAnalyzer(INPUT_JSON_PATH, OUTPUT_DIR, use_api_for_embeddings=True)\n",
        "    topic_model = analyzer.run_full_analysis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYKGUkcAkjIG"
      },
      "source": [
        "# 3有监督分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJblfHRK8Zz5"
      },
      "source": [
        "## 环境配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W6tstVwn8jYv"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic hdbscan umap-learn lightgbm shap transformers datasets nltk hiclass\n",
        "!pip install bertviz  # 用于注意力机制可视化（可选）\n",
        "\n",
        "# 下载NLTK资源\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "METxKW_38ilT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "SAVE_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/验证3+5/deberta-v3-base-local'\n",
        "\n",
        "print(f\"正在下载模型和分词器：'{MODEL_NAME}'...\")\n",
        "print(\"这是一个大型模型，可能需要几分钟时间...\")\n",
        "\n",
        "# 下载并保存分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "# 下载并保存模型\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "print(f\"\\n模型和分词器已成功保存到 '{SAVE_DIRECTORY}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1RlBV0dvtnu"
      },
      "source": [
        "## 下载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAouJz3smAGP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- UPDATED a---\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "SAVE_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/验证3+5/deberta-v3-base-local'\n",
        "# --- END UPDATE ---\n",
        "\n",
        "print(f\"Downloading model and tokenizer for '{MODEL_NAME}'...\")\n",
        "print(\"This is a large model and may take several minutes...\")\n",
        "\n",
        "# Download and save the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "# Download and save the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "print(f\"\\nModel and tokenizer saved successfully to '{SAVE_DIRECTORY}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWKegt2iQ7a"
      },
      "source": [
        "## 层次有监督分类模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv1CKm227OjI"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Hierarchical Supervised Classification with DeBERTa and LightGBM\n",
        "Enhanced with XAI via SHAP and Attention Visualization\n",
        "(Revised and Enhanced Version)\n",
        "\"\"\"\n",
        "\n",
        "# --- Environment Setup ---\n",
        "# Disable W&B logging to avoid prompts and unnecessary outputs\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# --- Core Imports ---\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import lightgbm as lgb\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Use a non-interactive backend for saving figures\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# --- Transformers Imports ---\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n",
        "    TrainingArguments, DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- Hierarchical Metrics ---\n",
        "# Note: Your custom implementation is clear and correct. For future projects,\n",
        "# the 'hiclass' library offers a standardized way to compute these metrics.\n",
        "def h_precision(y_true_paths, y_pred_paths):\n",
        "    \"\"\"Computes hierarchical precision.\"\"\"\n",
        "    correct_nodes = 0\n",
        "    total_predicted_nodes = 0\n",
        "    for true_path, pred_path in zip(y_true_paths, y_pred_paths):\n",
        "        true_set = set(true_path)\n",
        "        pred_set = set(pred_path)\n",
        "        correct_nodes += len(true_set.intersection(pred_set))\n",
        "        total_predicted_nodes += len(pred_set)\n",
        "    return correct_nodes / max(total_predicted_nodes, 1)\n",
        "\n",
        "def h_recall(y_true_paths, y_pred_paths):\n",
        "    \"\"\"Computes hierarchical recall.\"\"\"\n",
        "    correct_nodes = 0\n",
        "    total_true_nodes = 0\n",
        "    for true_path, pred_path in zip(y_true_paths, y_pred_paths):\n",
        "        true_set = set(true_path)\n",
        "        pred_set = set(pred_path)\n",
        "        correct_nodes += len(true_set.intersection(pred_set))\n",
        "        total_true_nodes += len(true_set)\n",
        "    return correct_nodes / max(total_true_nodes, 1)\n",
        "\n",
        "def h_f1(y_true_paths, y_pred_paths):\n",
        "    \"\"\"Computes hierarchical F1-score.\"\"\"\n",
        "    prec = h_precision(y_true_paths, y_pred_paths)\n",
        "    rec = h_recall(y_true_paths, y_pred_paths)\n",
        "    if prec + rec == 0:\n",
        "        return 0\n",
        "    return 2 * prec * rec / (prec + rec)\n",
        "\n",
        "# --- Attention Visualization (BertViz) ---\n",
        "try:\n",
        "    from bertviz import head_view, model_view\n",
        "    HAS_BERTVIZ = True\n",
        "except ImportError:\n",
        "    print(\"Warning: BertViz not installed. Attention visualization will be disabled.\")\n",
        "    HAS_BERTVIZ = False\n",
        "\n",
        "# --- Configuration Class ---\n",
        "class Config:\n",
        "    \"\"\"Configuration for the supervised classification pipeline\"\"\"\n",
        "    # Paths\n",
        "    # IMPORTANT: Please update this path to your actual file location\n",
        "    INPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/验证3+5/policies_with_embeddings_api.json'\n",
        "    OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/验证3+5/supervised_results'\n",
        "    TRANSFORMER_MODEL = 'microsoft/deberta-v3-base' # Using a model from Hub for easier access\n",
        "\n",
        "    # Split parameters\n",
        "    RANDOM_STATE = 42\n",
        "    TEST_SIZE = 0.2\n",
        "    VALIDATION_SIZE = 0.1 # This will be a fraction of the (1 - TEST_SIZE) data\n",
        "\n",
        "    # LightGBM parameters\n",
        "    LGBM_PARAMS = {\n",
        "        'objective': 'multiclass', 'metric': 'multi_logloss', 'n_estimators': 2000,\n",
        "        'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
        "        'num_leaves': 31, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'bagging_freq': 1,\n",
        "        'verbose': -1, 'n_jobs': -1, 'seed': RANDOM_STATE, 'boosting_type': 'gbdt',\n",
        "    }\n",
        "\n",
        "    # --- [FIXED] Transformer parameters ---\n",
        "    # Removed 'output_dir' from this dictionary to prevent the TypeError.\n",
        "    # The output directory is now set dynamically in the training function.\n",
        "    TRAINING_ARGS = {\n",
        "        'num_train_epochs': 8, 'learning_rate': 2e-5,\n",
        "        'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 16,\n",
        "        'gradient_accumulation_steps': 1, 'warmup_ratio': 0.1, 'weight_decay': 0.01,\n",
        "        'logging_dir': './logs', 'logging_steps': 50, 'evaluation_strategy': \"epoch\",\n",
        "        'save_strategy': \"epoch\", 'load_best_model_at_end': True,\n",
        "        'metric_for_best_model': 'f1', 'greater_is_better': True, 'report_to': \"none\"\n",
        "    }\n",
        "\n",
        "    # Hierarchy definition\n",
        "    HIERARCHY = {\n",
        "        'Guideline_Strategy': 'Macro',\n",
        "        'Planning_Layout': 'Macro',\n",
        "        'Institutional_Arrangements': 'Macro',\n",
        "        'Resource_Allocation_Policy': 'Micro',\n",
        "        'Innovation_Actor_Policy': 'Micro',\n",
        "        'Talent_Policy': 'Micro',\n",
        "        'Commercialization_Policy': 'Micro',\n",
        "        'Environment_Shaping_Policy': 'Micro',\n",
        "        'Macro': 'Root',\n",
        "        'Micro': 'Root',\n",
        "        'Root': None\n",
        "    }\n",
        "    MACRO_CLASSES = ['Guideline_Strategy', 'Planning_Layout', 'Institutional_Arrangements']\n",
        "\n",
        "# --- Custom Trainer for Weighted Loss ---\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"A custom trainer to apply class weights for handling imbalanced datasets.\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(self.train_dataset['label']),\n",
        "            y=np.array(self.train_dataset['label'])\n",
        "        )\n",
        "        weights_tensor = torch.tensor(class_weights, dtype=torch.float, device=model.device)\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# --- Main Classifier Class ---\n",
        "class HierarchicalClassifier:\n",
        "    \"\"\"Implements the complete hierarchical classification and XAI pipeline.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)\n",
        "        self.lgbm_models = {}\n",
        "        self.transformer_models = {}\n",
        "        self.label_encoders = {}\n",
        "        self.datasets = {}\n",
        "        self.load_and_prepare_data()\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Loads data, creates labels, and performs methodologically sound splits.\"\"\"\n",
        "        print(\"1. Loading and preparing data...\")\n",
        "        # A check to ensure the input file exists\n",
        "        if not os.path.exists(self.config.INPUT_JSON_PATH):\n",
        "            raise FileNotFoundError(f\"Input JSON not found at: {self.config.INPUT_JSON_PATH}. Please update the path in the Config class.\")\n",
        "\n",
        "        df = pd.read_json(self.config.INPUT_JSON_PATH)\n",
        "        df['text'] = df['NameEnglish'].fillna('') + \". \" + df['ShortDescription'].fillna('')\n",
        "        df = df[df['text'].str.len() > 10].reset_index(drop=True) # Basic cleaning\n",
        "\n",
        "        # Create primary (Macro/Micro) and secondary (8 classes) labels\n",
        "        df['primary_label_str'] = df['ClassificationLabel'].apply(lambda x: 'Macro' if x in self.config.MACRO_CLASSES else 'Micro')\n",
        "        self.df = df\n",
        "\n",
        "        # Encode all labels and store encoders\n",
        "        self.label_encoders['primary'] = LabelEncoder().fit(self.df['primary_label_str'])\n",
        "        self.label_encoders['secondary'] = LabelEncoder().fit(self.df['ClassificationLabel'])\n",
        "        self.df['primary_label'] = self.label_encoders['primary'].transform(self.df['primary_label_str'])\n",
        "        self.df['secondary_label'] = self.label_encoders['secondary'].transform(self.df['ClassificationLabel'])\n",
        "\n",
        "        # --- Methodologically Sound Data Splitting ---\n",
        "        # Split the main dataframe first\n",
        "        train_val_df, test_df = train_test_split(\n",
        "            self.df, test_size=self.config.TEST_SIZE, random_state=self.config.RANDOM_STATE,\n",
        "            stratify=self.df['secondary_label']\n",
        "        )\n",
        "        relative_val_size = self.config.VALIDATION_SIZE / (1 - self.config.TEST_SIZE)\n",
        "        train_df, val_df = train_test_split(\n",
        "            train_val_df, test_size=relative_val_size, random_state=self.config.RANDOM_STATE,\n",
        "            stratify=train_val_df['secondary_label']\n",
        "        )\n",
        "        self.datasets['main'] = {'train': train_df, 'val': val_df, 'test': test_df}\n",
        "        print(f\"Data Split (Main): Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "        # Create and split specialist datasets DERIVED from the main splits\n",
        "        for level in ['Macro', 'Micro']:\n",
        "            le = LabelEncoder().fit(self.df[self.df['primary_label_str'] == level]['ClassificationLabel'])\n",
        "            self.label_encoders[level.lower()] = le\n",
        "\n",
        "            self.datasets[level.lower()] = {\n",
        "                'train': train_df[train_df['primary_label_str'] == level].copy(),\n",
        "                'val': val_df[val_df['primary_label_str'] == level].copy(),\n",
        "                'test': test_df[test_df['primary_label_str'] == level].copy(),\n",
        "            }\n",
        "            # Add specialist label column to each split\n",
        "            for split in ['train', 'val', 'test']:\n",
        "                self.datasets[level.lower()][split]['specialist_label'] = le.transform(\n",
        "                    self.datasets[level.lower()][split]['ClassificationLabel']\n",
        "                )\n",
        "\n",
        "    def train_lgbm_models(self):\n",
        "        \"\"\"Trains the complete hierarchical pipeline for LightGBM.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n2. TRAINING LIGHTGBM HIERARCHICAL CLASSIFIERS\\n\" + \"=\"*50)\n",
        "        # Train dispatcher\n",
        "        self.lgbm_models['dispatcher'], _ = self._train_lgbm_node('main', 'primary_label', 'LGBM Dispatcher')\n",
        "        # Train specialists\n",
        "        self.lgbm_models['macro_specialist'], _ = self._train_lgbm_node('macro', 'specialist_label', 'LGBM Macro Specialist', True)\n",
        "        self.lgbm_models['micro_specialist'], _ = self._train_lgbm_node('micro', 'specialist_label', 'LGBM Micro Specialist', True)\n",
        "        # Evaluate\n",
        "        self.evaluate_hierarchical_pipeline('lightgbm')\n",
        "\n",
        "    def _train_lgbm_node(self, data_key, label_col, title, use_class_weight=False):\n",
        "        \"\"\"Helper to train a single LightGBM model node.\"\"\"\n",
        "        print(f\"\\n--- Training: {title} ---\")\n",
        "        dsets = self.datasets[data_key]\n",
        "        le = self.label_encoders['primary' if data_key == 'main' else data_key]\n",
        "        X_train = np.array(dsets['train']['embed'].tolist())\n",
        "        y_train = dsets['train'][label_col]\n",
        "        X_val = np.array(dsets['val']['embed'].tolist())\n",
        "        y_val = dsets['val'][label_col]\n",
        "        X_test = np.array(dsets['test']['embed'].tolist())\n",
        "        y_test = dsets['test'][label_col]\n",
        "\n",
        "        params = self.config.LGBM_PARAMS.copy()\n",
        "        params['objective'] = 'binary' if len(le.classes_) == 2 else 'multiclass'\n",
        "        params['metric'] = 'binary_logloss' if len(le.classes_) == 2 else 'multi_logloss'\n",
        "        if len(le.classes_) > 2: params['num_class'] = len(le.classes_)\n",
        "        if use_class_weight: params['class_weight'] = 'balanced'\n",
        "\n",
        "        model = lgb.LGBMClassifier(**params)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "        y_pred = model.predict(X_test)\n",
        "        metrics = self._evaluate_node(y_test, y_pred, le.classes_, title)\n",
        "        model.le_ = le # Attach encoder for later use\n",
        "        return model, metrics\n",
        "\n",
        "    def train_transformer_models(self):\n",
        "        \"\"\"Trains the complete hierarchical pipeline for DeBERTa.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n3. TRAINING TRANSFORMER (DEBERTA) CLASSIFIERS\\n\" + \"=\"*50)\n",
        "        if not torch.cuda.is_available(): print(\"WARNING: No CUDA device found. Training will be very slow.\")\n",
        "        # Train dispatcher\n",
        "        self.transformer_models['dispatcher'], _ = self._train_transformer_node('main', 'primary_label', 'DeBERTa Dispatcher')\n",
        "        # Train specialists\n",
        "        self.transformer_models['macro_specialist'], _ = self._train_transformer_node('macro', 'specialist_label', 'DeBERTa Macro Specialist', True)\n",
        "        self.transformer_models['micro_specialist'], _ = self._train_transformer_node('micro', 'specialist_label', 'DeBERTa Micro Specialist', True)\n",
        "        # Evaluate\n",
        "        self.evaluate_hierarchical_pipeline('transformer')\n",
        "\n",
        "    def _train_transformer_node(self, data_key, label_col, title, use_custom_trainer=False):\n",
        "        \"\"\"Helper to train a single Transformer model node.\"\"\"\n",
        "        print(f\"\\n--- Training: {title} ---\")\n",
        "        dsets = self.datasets[data_key]\n",
        "        le = self.label_encoders['primary' if data_key == 'main' else data_key]\n",
        "\n",
        "        train_ds = Dataset.from_pandas(dsets['train'][['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "        val_ds = Dataset.from_pandas(dsets['val'][['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "        test_ds = Dataset.from_pandas(dsets['test'][['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.config.TRANSFORMER_MODEL)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.config.TRANSFORMER_MODEL, num_labels=len(le.classes_), ignore_mismatched_sizes=True,\n",
        "            output_attentions=True # Ensure attentions are output for visualization\n",
        "        )\n",
        "\n",
        "        def tokenize(examples): return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "        train_ds, val_ds, test_ds = [ds.map(tokenize, batched=True) for ds in [train_ds, val_ds, test_ds]]\n",
        "\n",
        "        def compute_metrics(p):\n",
        "            preds = np.argmax(p.predictions, axis=1)\n",
        "            return {'f1': f1_score(p.label_ids, preds, average='macro'), 'accuracy': accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "        # The TrainingArguments object is now created correctly without the duplicate 'output_dir'\n",
        "        args = TrainingArguments(\n",
        "            output_dir=f\"{self.config.OUTPUT_DIR}/checkpoints/{title.replace(' ', '_')}\",\n",
        "            **self.config.TRAINING_ARGS\n",
        "        )\n",
        "        TrainerClass = CustomTrainer if use_custom_trainer else Trainer\n",
        "        trainer = TrainerClass(\n",
        "            model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "            tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "            compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        preds = trainer.predict(test_ds)\n",
        "        y_pred = np.argmax(preds.predictions, axis=-1)\n",
        "        metrics = self._evaluate_node(test_ds['label'], y_pred, le.classes_, title)\n",
        "        trainer.le_ = le # Attach encoder\n",
        "        return trainer, metrics\n",
        "\n",
        "    def evaluate_hierarchical_pipeline(self, model_type):\n",
        "        \"\"\"Evaluates the complete hierarchical pipeline for a given model type.\"\"\"\n",
        "        print(f\"\\n--- 4. Evaluating End-to-End Pipeline for: {model_type.upper()} ---\")\n",
        "        models = self.lgbm_models if model_type == 'lightgbm' else self.transformer_models\n",
        "        test_df = self.datasets['main']['test']\n",
        "        y_true_final = test_df['secondary_label']\n",
        "        y_pred_final, y_true_paths, y_pred_paths = [], [], []\n",
        "\n",
        "        for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=f\"Predicting with {model_type}\"):\n",
        "            # Predict primary label\n",
        "            primary_pred_label, _ = self._predict_node(models['dispatcher'], row, model_type)\n",
        "            # Select specialist\n",
        "            specialist = models[f\"{primary_pred_label.lower()}_specialist\"]\n",
        "            # Predict secondary label\n",
        "            final_pred_label, _ = self._predict_node(specialist, row, model_type)\n",
        "            # Append results\n",
        "            y_pred_final.append(self.label_encoders['secondary'].transform([final_pred_label])[0])\n",
        "            true_label = row['ClassificationLabel']\n",
        "            # This line is now safe because HIERARCHY keys match ClassificationLabel values\n",
        "            y_true_paths.append([self.config.HIERARCHY[true_label], true_label])\n",
        "            y_pred_paths.append([primary_pred_label, final_pred_label])\n",
        "\n",
        "        # Standard Evaluation\n",
        "        self._evaluate_node(y_true_final, y_pred_final, self.label_encoders['secondary'].classes_, f\"End-to-End Pipeline ({model_type.upper()})\")\n",
        "        # Hierarchical Evaluation\n",
        "        h_p, h_r, h_f1_score = h_precision(y_true_paths, y_pred_paths), h_recall(y_true_paths, y_pred_paths), h_f1(y_true_paths, y_pred_paths)\n",
        "        print(\"\\n--- Hierarchical Evaluation Metrics ---\")\n",
        "        print(f\"Hierarchical Precision: {h_p:.4f}\\nHierarchical Recall: {h_r:.4f}\\nHierarchical F1 Score: {h_f1_score:.4f}\")\n",
        "\n",
        "    def _predict_node(self, model, row, model_type):\n",
        "        \"\"\"Helper to get a prediction from a single model node.\"\"\"\n",
        "        if model_type == 'lightgbm':\n",
        "            features = np.array(row['embed']).reshape(1, -1)\n",
        "            pred_encoded = model.predict(features)[0]\n",
        "        else: # transformer\n",
        "            inputs = model.tokenizer(row['text'], return_tensors=\"pt\", truncation=True, max_length=512).to(model.model.device)\n",
        "            with torch.no_grad():\n",
        "                logits = model.model(**inputs).logits\n",
        "            pred_encoded = torch.argmax(logits, dim=1).item()\n",
        "        return model.le_.inverse_transform([pred_encoded])[0], pred_encoded\n",
        "\n",
        "    def analyze_explainability(self):\n",
        "        \"\"\"Performs the full XAI analysis pipeline for the transformer models.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n5. PERFORMING EXPLAINABILITY ANALYSIS (SHAP & ATTENTION)\\n\" + \"=\"*50)\n",
        "        if not self.transformer_models:\n",
        "            print(\"No transformer models available for XAI analysis.\")\n",
        "            return\n",
        "\n",
        "        # Use a small, representative subset of the test data for explanations\n",
        "        sample_df = self.datasets['main']['test'].sample(20, random_state=self.config.RANDOM_STATE)\n",
        "\n",
        "        # --- SHAP Analysis ---\n",
        "        print(\"\\n--- Generating SHAP Explanations ---\")\n",
        "        # Create a single SHAP explainer for each model to reuse\n",
        "        shap_explainers = {\n",
        "            name: shap.Explainer(self._get_prediction_function(model), model.tokenizer)\n",
        "            for name, model in self.transformer_models.items()\n",
        "        }\n",
        "\n",
        "        # Global Explanations (Summary Plots)\n",
        "        for name, model in self.transformer_models.items():\n",
        "            print(f\"Generating global SHAP plot for {name}...\")\n",
        "            # Use the appropriate test set for each model\n",
        "            if name == 'dispatcher':\n",
        "                data_key = 'main'\n",
        "            else:\n",
        "                data_key = name.split('_')[0]\n",
        "\n",
        "            texts = self.datasets[data_key]['test']['text'].tolist()\n",
        "            # Use a manageable sample size for SHAP to avoid excessive computation time\n",
        "            sample_size = min(50, len(texts))\n",
        "            shap_values = shap_explainers[name](texts[:sample_size])\n",
        "            plt.figure()\n",
        "            shap.summary_plot(shap_values, plot_type='bar', class_names=model.le_.classes_, show=False)\n",
        "            plt.title(f\"SHAP Feature Importance - {name}\")\n",
        "            plt.savefig(f\"{self.config.OUTPUT_DIR}/shap_global_{name}.png\", bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "        # Local Explanations (for specific examples)\n",
        "        print(\"\\nGenerating local SHAP explanations for one example per class...\")\n",
        "        for label_str in self.label_encoders['secondary'].classes_:\n",
        "            example_row = self.df[self.df['ClassificationLabel'] == label_str].sample(1, random_state=self.config.RANDOM_STATE).iloc[0]\n",
        "            text = example_row['text']\n",
        "            primary_label = example_row['primary_label_str']\n",
        "\n",
        "            # Explain dispatcher\n",
        "            self._explain_local_shap(shap_explainers['dispatcher'], text, f\"{label_str}_as_dispatcher\")\n",
        "            # Explain specialist\n",
        "            specialist_name = f\"{primary_label.lower()}_specialist\"\n",
        "            self._explain_local_shap(shap_explainers[specialist_name], text, f\"{label_str}_as_{specialist_name}\")\n",
        "\n",
        "        # --- Attention Visualization ---\n",
        "        if HAS_BERTVIZ:\n",
        "            print(\"\\n--- Visualizing Attention Patterns ---\")\n",
        "            example_text = sample_df.iloc[0]['text']\n",
        "            for name, model in self.transformer_models.items():\n",
        "                self._visualize_attention(model, example_text, name)\n",
        "\n",
        "    def _get_prediction_function(self, trainer):\n",
        "        \"\"\"Creates a prediction function compatible with SHAP.\"\"\"\n",
        "        model = trainer.model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.eval()\n",
        "        def predict(texts):\n",
        "            inputs = trainer.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                logits = model(**inputs).logits\n",
        "            return torch.softmax(logits, dim=-1).cpu().numpy() # SHAP works best with probabilities\n",
        "        return predict\n",
        "\n",
        "    def _explain_local_shap(self, explainer, text, filename):\n",
        "        \"\"\"Generates and saves a single local SHAP text plot.\"\"\"\n",
        "        try:\n",
        "            shap_values = explainer([text])\n",
        "            # Using shap.plots.text requires careful handling for saving\n",
        "            # We will generate the plot and save it with matplotlib\n",
        "            shap.plots.text(shap_values, show=False)\n",
        "            plt.savefig(f\"{self.config.OUTPUT_DIR}/shap_local_{filename}.png\", bbox_inches='tight', dpi=150)\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not generate SHAP plot for {filename}: {e}\")\n",
        "\n",
        "    def _visualize_attention(self, trainer, text, model_name):\n",
        "        \"\"\"Generates and saves attention visualizations.\"\"\"\n",
        "        print(f\"Visualizing attention for {model_name}...\")\n",
        "        model = trainer.model\n",
        "        tokenizer = trainer.tokenizer\n",
        "        try:\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "            # The model must be loaded with output_attentions=True\n",
        "            outputs = model(**inputs)\n",
        "            html_head_view = head_view(outputs.attentions, tokens, html_action='return_string')\n",
        "            with open(f\"{self.config.OUTPUT_DIR}/attention_{model_name}_head_view.html\", 'w', encoding='utf-8') as f:\n",
        "                f.write(html_head_view)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not visualize attention for {model_name}: {e}\")\n",
        "\n",
        "    def _evaluate_node(self, y_true, y_pred, labels, title):\n",
        "        \"\"\"Helper to evaluate a single node and save a confusion matrix.\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        print(f\"\\n--- Evaluation: {title} ---\\nAccuracy: {accuracy:.4f} | Macro F1: {f1:.4f}\")\n",
        "        print(classification_report(y_true, y_pred, target_names=labels, zero_division=0))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(max(8, len(labels)), max(6, len(labels)*0.8)))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "        plt.title(f'Confusion Matrix - {title}')\n",
        "        plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
        "        filename = f\"{self.config.OUTPUT_DIR}/cm_{title.replace(' ', '_')}.png\"\n",
        "        plt.savefig(filename, bbox_inches='tight'); plt.close()\n",
        "        return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "    def run_full_pipeline(self):\n",
        "        \"\"\"Runs the entire supervised learning pipeline.\"\"\"\n",
        "        self.train_lgbm_models()\n",
        "        self.train_transformer_models()\n",
        "        self.analyze_explainability()\n",
        "        print(\"\\nSupervised classification and XAI pipeline complete!\")\n",
        "        print(f\"All results and visualizations saved to {self.config.OUTPUT_DIR}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        config = Config()\n",
        "        classifier = HierarchicalClassifier(config)\n",
        "        classifier.run_full_pipeline()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during pipeline execution: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

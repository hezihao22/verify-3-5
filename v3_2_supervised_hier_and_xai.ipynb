{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0zCvuD4JRdg"
      },
      "source": [
        "# v3_2 层次有监督分类（LightGBM + DeBERTa）与可解释性\n",
        "\n",
        "要点：\n",
        "- 统一文本字段：NameEnglish + ShortDescription；\n",
        "- 复用 v3_1 的 4096 维嵌入或自行读取；\n",
        "- 分层训练：Dispatcher (Macro/Micro) + Specialists（Macro三类、Micro五类）\n",
        "- 强化可解释性：\n",
        "  - LightGBM：SHAP 全局/局部 + 类别 c-TF-IDF 词特征对照；\n",
        "  - DeBERTa：Integrated Gradients token 归因并导出着色 HTML；\n",
        "- 明确保留并输出 v1 风格的混淆矩阵（热力图），包括：dispatcher、两个 specialist，以及端到端管道。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup2"
      },
      "outputs": [],
      "source": [
        "import sys, subprocess\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "for p in ['pandas','numpy','scikit-learn','matplotlib','seaborn','lightgbm','transformers','datasets','shap','captum','nltk']:\n",
        "    try:\n",
        "        __import__(p)\n",
        "    except Exception:\n",
        "        pip_install(p)\n",
        "\n",
        "import os, json, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import torch\n",
        "from captum.attr import IntegratedGradients\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
        "from datasets import Dataset\n",
        "\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "# 路径配置（请按需修改）\n",
        "CTM_OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/验证3+5/v3_ctm_results'\n",
        "EMBEDDED_JSON_PATH = os.path.join(CTM_OUTPUT_DIR, 'policies_with_embeddings_api_4096.json')\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/验证3+5/v3_supervised_results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# 分类层级定义（8 类 -> 宏/微 两层）\n",
        "MACRO_CLASSES = ['Guideline_Strategy', 'Planning_Layout', 'Institutional_Arrangements']\n",
        "ALL_CLASSES = [\n",
        "    'Guideline_Strategy', 'Planning_Layout', 'Institutional_Arrangements',\n",
        "    'Resource_Allocation_Policy', 'Innovation_Actor_Policy', 'Talent_Policy',\n",
        "    'Commercialization_Policy', 'Environment_Shaping_Policy'\n",
        "]\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data"
      },
      "outputs": [],
      "source": [
        "# 1) 读入数据，构造文本与标签（统一 NameEnglish），并划分数据集\n",
        "\n",
        "def load_data(path):\n",
        "    df = pd.read_json(path)\n",
        "    df['text'] = (df.get('NameEnglish','').fillna('') + '. ' + df.get('ShortDescription','').fillna('')).astype(str)\n",
        "    df = df[df['text'].str.len() > 10].reset_index(drop=True)\n",
        "    # 需要存在标注列 ClassificationLabel\n",
        "    if 'ClassificationLabel' not in df.columns:\n",
        "        raise ValueError('数据缺少 ClassificationLabel 列。请使用带有 8 类标注的数据文件。')\n",
        "    return df\n",
        "\n",
        "df = load_data(EMBEDDED_JSON_PATH)\n",
        "df['primary_label_str'] = df['ClassificationLabel'].apply(lambda x: 'Macro' if x in MACRO_CLASSES else 'Micro')\n",
        "\n",
        "le_primary = LabelEncoder().fit(df['primary_label_str'])\n",
        "le_secondary = LabelEncoder().fit(df['ClassificationLabel'])\n",
        "df['primary_label'] = le_primary.transform(df['primary_label_str'])\n",
        "df['secondary_label'] = le_secondary.transform(df['ClassificationLabel'])\n",
        "\n",
        "train_val_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE, stratify=df['secondary_label'])\n",
        "relative_val = 0.1 / 0.8\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=relative_val, random_state=RANDOM_STATE, stratify=train_val_df['secondary_label'])\n",
        "print(f'Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}')\n",
        "\n",
        "# Specialist 数据集（从主划分衍生）\n",
        "macro_train = train_df[train_df['primary_label_str']=='Macro'].copy()\n",
        "macro_val   = val_df[val_df['primary_label_str']=='Macro'].copy()\n",
        "macro_test  = test_df[test_df['primary_label_str']=='Macro'].copy()\n",
        "micro_train = train_df[train_df['primary_label_str']=='Micro'].copy()\n",
        "micro_val   = val_df[val_df['primary_label_str']=='Micro'].copy()\n",
        "micro_test  = test_df[test_df['primary_label_str']=='Micro'].copy()\n",
        "\n",
        "le_macro = LabelEncoder().fit(macro_train['ClassificationLabel'])\n",
        "le_micro = LabelEncoder().fit(micro_train['ClassificationLabel'])\n",
        "\n",
        "for split, df_ in [('train', macro_train), ('val', macro_val), ('test', macro_test)]:\n",
        "    df_[f'specialist_label'] = le_macro.transform(df_['ClassificationLabel'])\n",
        "for split, df_ in [('train', micro_train), ('val', micro_val), ('test', micro_test)]:\n",
        "    df_[f'specialist_label'] = le_micro.transform(df_['ClassificationLabel'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_helpers"
      },
      "outputs": [],
      "source": [
        "# 2) 评估与可视化（保留 v1 风格的混淆矩阵热力图）\n",
        "\n",
        "def evaluate_and_plot(y_true, y_pred, labels, title):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1  = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    print(f'\\n=== {title} ===\\nAccuracy={acc:.4f} | Macro-F1={f1:.4f}')\n",
        "    print(classification_report(y_true, y_pred, target_names=labels, zero_division=0))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(max(8, len(labels)*0.9), max(6, len(labels)*0.7)))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
        "    fn = os.path.join(OUTPUT_DIR, f'cm_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", title)}.png')\n",
        "    plt.savefig(fn, bbox_inches='tight'); plt.close()\n",
        "    print('混淆矩阵已保存：', fn)\n",
        "    return {'accuracy': acc, 'f1': f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgbm"
      },
      "outputs": [],
      "source": [
        "# 3) LightGBM 分层训练（使用 4096 维嵌入），含早停与类权重\n",
        "\n",
        "LGBM_BASE = {\n",
        "    'learning_rate': 0.02,\n",
        "    'n_estimators': 5000,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'num_leaves': 31,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 0.1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': RANDOM_STATE,\n",
        "    'boosting_type': 'gbdt',\n",
        "}\n",
        "\n",
        "def train_lgbm(X_train, y_train, X_val, y_val, num_classes, title, use_balanced=True):\n",
        "    params = LGBM_BASE.copy()\n",
        "    if num_classes == 2:\n",
        "        params.update({'objective': 'binary', 'metric': 'binary_logloss'})\n",
        "    else:\n",
        "        params.update({'objective': 'multiclass', 'metric': 'multi_logloss', 'num_class': num_classes})\n",
        "    if use_balanced:\n",
        "        params['class_weight'] = 'balanced'\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        callbacks=[lgb.early_stopping(200, verbose=False)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Dispatcher\n",
        "Xd_tr = np.array(train_df['embed'].tolist());    yd_tr = train_df['primary_label'].values\n",
        "Xd_va = np.array(val_df['embed'].tolist());      yd_va = val_df['primary_label'].values\n",
        "Xd_te = np.array(test_df['embed'].tolist());     yd_te = test_df['primary_label'].values\n",
        "\n",
        "lgbm_dispatcher = train_lgbm(Xd_tr, yd_tr, Xd_va, yd_va, len(le_primary.classes_), 'LGBM Dispatcher (Macro-Micro)')\n",
        "pred_d = lgbm_dispatcher.predict(Xd_te)\n",
        "_ = evaluate_and_plot(yd_te, pred_d, le_primary.classes_, 'LGBM Dispatcher (Macro-Micro)')\n",
        "\n",
        "# Macro Specialist\n",
        "Xm_tr = np.array(macro_train['embed'].tolist()); ym_tr = macro_train['specialist_label'].values\n",
        "Xm_va = np.array(macro_val['embed'].tolist());   ym_va = macro_val['specialist_label'].values\n",
        "Xm_te = np.array(macro_test['embed'].tolist());  ym_te = macro_test['specialist_label'].values\n",
        "lgbm_macro = train_lgbm(Xm_tr, ym_tr, Xm_va, ym_va, len(le_macro.classes_), 'LGBM Macro Specialist')\n",
        "pred_m = lgbm_macro.predict(Xm_te)\n",
        "_ = evaluate_and_plot(ym_te, pred_m, le_macro.classes_, 'LGBM Macro Specialist')\n",
        "\n",
        "# Micro Specialist\n",
        "xmi_tr = np.array(micro_train['embed'].tolist()); ymi_tr = micro_train['specialist_label'].values\n",
        "xmi_va = np.array(micro_val['embed'].tolist());   ymi_va = micro_val['specialist_label'].values\n",
        "xmi_te = np.array(micro_test['embed'].tolist());  ymi_te = micro_test['specialist_label'].values\n",
        "lgbm_micro = train_lgbm(xmi_tr, ymi_tr, xmi_va, ymi_va, len(le_micro.classes_), 'LGBM Micro Specialist')\n",
        "pred_mi = lgbm_micro.predict(xmi_te)\n",
        "_ = evaluate_and_plot(ymi_te, pred_mi, le_micro.classes_, 'LGBM Micro Specialist')\n",
        "\n",
        "# 端到端：先 Dispatcher，再调用对应 Specialist\n",
        "y_true_final = test_df['secondary_label'].values\n",
        "y_pred_final = []\n",
        "for i, row in test_df.iterrows():\n",
        "    feat = np.array(row['embed']).reshape(1, -1)\n",
        "    p_primary = lgbm_dispatcher.predict(feat)[0]\n",
        "    primary_label = le_primary.inverse_transform([p_primary])[0]\n",
        "    if primary_label == 'Macro':\n",
        "        p_spec = lgbm_macro.predict(feat)[0]\n",
        "        final_label = le_macro.inverse_transform([p_spec])[0]\n",
        "    else:\n",
        "        p_spec = lgbm_micro.predict(feat)[0]\n",
        "        final_label = le_micro.inverse_transform([p_spec])[0]\n",
        "    y_pred_final.append(le_secondary.transform([final_label])[0])\n",
        "evaluate_and_plot(y_true_final, y_pred_final, le_secondary.classes_, 'End-to-End Pipeline (LGBM)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xai_lgbm"
      },
      "outputs": [],
      "source": [
        "# 4) LightGBM 可解释性：SHAP 全局/局部 + 类别 c-TF-IDF 词对照（辅助）\n",
        "\n",
        "def shap_global_bar(model, X, class_names, name):\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    # 采样以控制开销\n",
        "    X_sample = X[:min(200, X.shape[0])]\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "    # 多分类：shap_values 是 list\n",
        "    plt.figure()\n",
        "    try:\n",
        "        # 汇总为平均|SHAP|并排序，画前 20 维\n",
        "        if isinstance(shap_values, list):\n",
        "            mean_abs = np.mean([np.mean(np.abs(sv), axis=0) for sv in shap_values], axis=0)\n",
        "        else:\n",
        "            mean_abs = np.mean(np.abs(shap_values), axis=0)\n",
        "        idx = np.argsort(-mean_abs)[:20]\n",
        "        plt.bar(range(len(idx)), mean_abs[idx])\n",
        "        plt.xticks(range(len(idx)), [f'dim_{i}' for i in idx], rotation=45, ha='right')\n",
        "        plt.title(f'SHAP Global Importance (Top 20) - {name}')\n",
        "        fn = os.path.join(OUTPUT_DIR, f'shap_global_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", name)}.png')\n",
        "        plt.tight_layout(); plt.savefig(fn, dpi=150); plt.close()\n",
        "        print('保存 SHAP 全局重要度图：', fn)\n",
        "    except Exception as e:\n",
        "        print('SHAP 全局绘制失败：', e)\n",
        "\n",
        "# Dispatcher 全局 SHAP\n",
        "shap_global_bar(lgbm_dispatcher, Xd_te, le_primary.classes_, 'LGBM Dispatcher (Macro-Micro)')\n",
        "# Specialist 全局 SHAP\n",
        "shap_global_bar(lgbm_macro, Xm_te, le_macro.classes_, 'LGBM Macro Specialist')\n",
        "shap_global_bar(lgbm_micro, xmi_te, le_micro.classes_, 'LGBM Micro Specialist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deberta"
      },
      "outputs": [],
      "source": [
        "# 5) DeBERTa 分层训练（含早停），并输出混淆矩阵\n",
        "\n",
        "TRANSFORMER_MODEL = 'microsoft/deberta-v3-base'\n",
        "\n",
        "def train_transformer(text_series, y, text_series_val, y_val, num_labels, title, class_names):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        TRANSFORMER_MODEL, num_labels=num_labels\n",
        "    )\n",
        "    train_ds = Dataset.from_dict({'text': text_series.tolist(), 'label': y.tolist()})\n",
        "    val_ds   = Dataset.from_dict({'text': text_series_val.tolist(), 'label': y_val.tolist()})\n",
        "    def tok_fn(batch):\n",
        "        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
        "    train_ds = train_ds.map(tok_fn, batched=True)\n",
        "    val_ds   = val_ds.map(tok_fn, batched=True)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=os.path.join(OUTPUT_DIR, 'ckpt', re.sub(r'[^a-zA-Z0-9_]+','_', title)),\n",
        "        num_train_epochs=8,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='f1',\n",
        "        report_to='none'\n",
        "    )\n",
        "    def compute_metrics(p):\n",
        "        preds = np.argmax(p.predictions, axis=1)\n",
        "        return {\n",
        "            'accuracy': accuracy_score(p.label_ids, preds),\n",
        "            'f1': f1_score(p.label_ids, preds, average='macro', zero_division=0)\n",
        "        }\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer),\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    trainer.train()\n",
        "    return trainer\n",
        "\n",
        "# Dispatcher\n",
        "tr_dispatcher = train_transformer(train_df['text'], train_df['primary_label'], val_df['text'], val_df['primary_label'], len(le_primary.classes_), 'DeBERTa Dispatcher', le_primary.classes_)\n",
        "test_ds_d = Dataset.from_dict({'text': test_df['text'].tolist(), 'label': test_df['primary_label'].tolist()})\n",
        "tok_d = tr_dispatcher.tokenizer\n",
        "test_ds_d = test_ds_d.map(lambda b: tok_d(b['text'], padding='max_length', truncation=True, max_length=512), batched=True)\n",
        "pred_d_logits = tr_dispatcher.predict(test_ds_d).predictions\n",
        "pred_d_label  = np.argmax(pred_d_logits, axis=1)\n",
        "evaluate_and_plot(test_df['primary_label'].values, pred_d_label, le_primary.classes_, 'DeBERTa Dispatcher (Macro-Micro)')\n",
        "\n",
        "# Macro Specialist\n",
        "tr_macro = train_transformer(macro_train['text'], macro_train['specialist_label'], macro_val['text'], macro_val['specialist_label'], len(le_macro.classes_), 'DeBERTa Macro Specialist', le_macro.classes_)\n",
        "test_ds_m = Dataset.from_dict({'text': macro_test['text'].tolist(), 'label': macro_test['specialist_label'].tolist()})\n",
        "tok_m = tr_macro.tokenizer\n",
        "test_ds_m = test_ds_m.map(lambda b: tok_m(b['text'], padding='max_length', truncation=True, max_length=512), batched=True)\n",
        "pred_m_logits = tr_macro.predict(test_ds_m).predictions\n",
        "pred_m_label  = np.argmax(pred_m_logits, axis=1)\n",
        "evaluate_and_plot(macro_test['specialist_label'].values, pred_m_label, le_macro.classes_, 'DeBERTa Macro Specialist')\n",
        "\n",
        "# Micro Specialist\n",
        "tr_micro = train_transformer(micro_train['text'], micro_train['specialist_label'], micro_val['text'], micro_val['specialist_label'], len(le_micro.classes_), 'DeBERTa Micro Specialist', le_micro.classes_)\n",
        "test_ds_mi = Dataset.from_dict({'text': micro_test['text'].tolist(), 'label': micro_test['specialist_label'].tolist()})\n",
        "tok_mi = tr_micro.tokenizer\n",
        "test_ds_mi = test_ds_mi.map(lambda b: tok_mi(b['text'], padding='max_length', truncation=True, max_length=512), batched=True)\n",
        "pred_mi_logits = tr_micro.predict(test_ds_mi).predictions\n",
        "pred_mi_label  = np.argmax(pred_mi_logits, axis=1)\n",
        "evaluate_and_plot(micro_test['specialist_label'].values, pred_mi_label, le_micro.classes_, 'DeBERTa Micro Specialist')\n",
        "\n",
        "# 端到端（DeBERTa）\n",
        "y_true_final = test_df['secondary_label'].values\n",
        "y_pred_final = []\n",
        "device = tr_dispatcher.model.device\n",
        "for i, row in test_df.iterrows():\n",
        "    # dispatcher\n",
        "    inputs = tok_d(row['text'], return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = tr_dispatcher.model(**inputs).logits\n",
        "    p_primary = torch.argmax(logits, dim=1).item()\n",
        "    primary_label = le_primary.inverse_transform([p_primary])[0]\n",
        "    # specialist\n",
        "    if primary_label == 'Macro':\n",
        "        tok_s = tok_m; mdl_s = tr_macro.model\n",
        "        le_s = le_macro\n",
        "    else:\n",
        "        tok_s = tok_mi; mdl_s = tr_micro.model\n",
        "        le_s = le_micro\n",
        "    inp2 = tok_s(row['text'], return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        log2 = mdl_s(**inp2).logits\n",
        "    p_spec = torch.argmax(log2, dim=1).item()\n",
        "    final_label_str = le_s.inverse_transform([p_spec])[0]\n",
        "    y_pred_final.append(le_secondary.transform([final_label_str])[0])\n",
        "evaluate_and_plot(y_true_final, y_pred_final, le_secondary.classes_, 'End-to-End Pipeline (DeBERTa)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig"
      },
      "outputs": [],
      "source": [
        "# 6) DeBERTa 可解释性：Integrated Gradients（单样本 token 归因，导出 HTML）\n",
        "\n",
        "def ig_explain_to_html(trainer, text, outfile):\n",
        "    model = trainer.model\n",
        "    tokenizer = trainer.tokenizer\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=256)\n",
        "    for k in inputs:\n",
        "        inputs[k] = inputs[k].to(model.device)\n",
        "    ig = IntegratedGradients(lambda input_ids: model(input_ids=input_ids, attention_mask=inputs['attention_mask']).logits)\n",
        "    baseline = torch.zeros_like(inputs['input_ids']).to(model.device)\n",
        "    attributions, _ = ig.attribute(inputs['input_ids'], baselines=baseline, target=None, return_convergence_delta=True)\n",
        "    atts = attributions.squeeze(0).sum(dim=-1).detach().cpu().numpy()\n",
        "    toks = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].detach().cpu().numpy())\n",
        "    # 归一化\n",
        "    if len(atts) > 0:\n",
        "        m = np.max(np.abs(atts));\n",
        "        if m > 0: atts = atts / m\n",
        "    # 简易 HTML 着色\n",
        "    def color(v):\n",
        "        # 红色正贡献，蓝色负贡献\n",
        "        r = int(max(0, v)*255); b = int(max(0, -v)*255); g = 0\n",
        "        return f'rgb({r},{g},{b})'\n",
        "    spans = []\n",
        "    for t, a in zip(toks, atts):\n",
        "        if t in ['[CLS]','[SEP]','[PAD]']:\n",
        "            continue\n",
        "        spans.append(f'<span style=\"background-color:{color(a)};color:#000;padding:2px;margin:1px;display:inline-block;border-radius:3px;\">{t}</span>')\n",
        "    html = f\"<html><meta charset='utf-8'><body><div>{''.join(spans)}</div></body></html>\"\n",
        "    with open(outfile, 'w', encoding='utf-8') as f:\n",
        "        f.write(html)\n",
        "    print('IG 可视化已保存：', outfile)\n",
        "\n",
        "# 对每个 specialist 选一个测试样本示例\n",
        "if len(macro_test) > 0:\n",
        "    ex_text = macro_test.iloc[0]['text']\n",
        "    ig_explain_to_html(tr_macro, ex_text, os.path.join(OUTPUT_DIR, 'ig_macro_example.html'))\n",
        "if len(micro_test) > 0:\n",
        "    ex_text2 = micro_test.iloc[0]['text']\n",
        "    ig_explain_to_html(tr_micro, ex_text2, os.path.join(OUTPUT_DIR, 'ig_micro_example.html'))\n",
        "\n",
        "print('全部训练与可解释性分析完成。结果目录：', OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGLYoUEkJRdl"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# 说明：\n",
        "# 将本文件中各段代码按顺序粘贴为 v3_2_supervised_hier_and_xai.ipynb 的新增单元格（放在训练与评估完成之后），\n",
        "# 即可在 v3_2 中集成：\n",
        "# 1) c-TF-IDF 类词对照（宏/微 与 8 类）\n",
        "# 2) 可靠性（温度标定）与可靠性图（Reliability Diagram）、ECE 指标（LightGBM 与 DeBERTa）\n",
        "# 3) “3+5 vs 2+6” 对比图（读取 v3_1 的 grid_{macro,micro}.json 指标，绘制与标注 K=3/5/2/6）\n",
        "#\n",
        "# 依赖：numpy, pandas, scikit-learn, matplotlib, seaborn, nltk, datasets, transformers, torch\n",
        "# 运行前提：v3_2 中已定义 df、train_df/val_df/test_df、宏/微拆分数据集、lgbm_* 模型与 tr_* 训练器、OUTPUT_DIR 等变量。\n",
        "# 路径前提：若已运行 v3_1，CTM_OUTPUT_DIR 下有 grid_macro.json 与 grid_micro.json（用于 3+5 vs 2+6 绘图）。\n",
        "#\n",
        "# 注意：以下代码对缺失对象做了存在性检查，若某些变量在你的 v3_2 中命名不同，请按需替换。\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "try:\n",
        "    _ = stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "# =========================\n",
        "# 一、c-TF-IDF 类词对照\n",
        "# =========================\n",
        "\n",
        "def compute_c_tf_idf_per_class(texts, labels, top_n=20, stop_words_extra=None, min_df=3, max_df=0.9):\n",
        "    \"\"\"\n",
        "    计算 c-TF-IDF（Class-based TF-IDF），用于每个类别的代表词和权重：\n",
        "    - 将同一类别的所有文本拼接为“类文档”\n",
        "    - 对类文档做词袋统计，计算 TF（类内归一化）与类级 IDF（基于类文档数）\n",
        "    - c-TF-IDF = TF * IDF\n",
        "    返回：dict 类别 -> DataFrame(columns=[term, score])\n",
        "    \"\"\"\n",
        "    # 1) 将文本按类别聚合为类文档\n",
        "    df_tmp = pd.DataFrame({'text': texts, 'label': labels})\n",
        "    grouped = df_tmp.groupby('label')['text'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
        "    class_docs = grouped['text'].tolist()\n",
        "    class_names = grouped['label'].tolist()\n",
        "\n",
        "    # 2) 构造停用词\n",
        "    base_sw = set(stopwords.words('english'))\n",
        "    extra = set(stop_words_extra or [])\n",
        "    sw = list(base_sw | extra)\n",
        "\n",
        "    # 3) 词袋（对类文档进行向量化）\n",
        "    vectorizer = CountVectorizer(stop_words=sw, min_df=min_df, max_df=max_df)\n",
        "    X = vectorizer.fit_transform(class_docs)  # 形状：[n_classes, vocab_size]\n",
        "    vocab = np.array(vectorizer.get_feature_names_out())\n",
        "    X = X.toarray().astype(float)\n",
        "\n",
        "    # 若全 0（极端小样本类），避免除零\n",
        "    if X.sum() == 0:\n",
        "        return {cn: pd.DataFrame(columns=['term', 'score']) for cn in class_names}\n",
        "\n",
        "    n_classes = X.shape[0]\n",
        "\n",
        "    # 4) 类内 TF：每行（类文档）归一化\n",
        "    row_sums = X.sum(axis=1, keepdims=True) + 1e-12\n",
        "    tf = X / row_sums\n",
        "\n",
        "    # 5) 类级 DF 与 IDF\n",
        "    df_class = (X > 0).sum(axis=0)  # 出现在多少个类文档里\n",
        "    idf = np.log(1 + n_classes / (df_class + 1e-12))\n",
        "\n",
        "    # 6) c-TF-IDF\n",
        "    c_tfidf = tf * idf\n",
        "\n",
        "    # 7) 为每个类别选 Top-N 词\n",
        "    results = {}\n",
        "    for i, cn in enumerate(class_names):\n",
        "        scores = c_tfidf[i]\n",
        "        idx = np.argsort(-scores)[:top_n]\n",
        "        results[cn] = pd.DataFrame({'term': vocab[idx], 'score': scores[idx]})\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_ctfidf_bars(ctfidf_dict, title_prefix, out_dir):\n",
        "    \"\"\"\n",
        "    将 c-TF-IDF 结果绘制为柱状图，每个类别一张。\n",
        "    \"\"\"\n",
        "    save_dir = os.path.join(out_dir, 'ctfidf')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    for cls, df_cls in ctfidf_dict.items():\n",
        "        if df_cls.empty:\n",
        "            continue\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.bar(df_cls['term'], df_cls['score'], color='#4C72B0')\n",
        "        plt.title(f'{title_prefix} - {cls}')\n",
        "        plt.xticks(rotation=60, ha='right')\n",
        "        plt.ylabel('c-TF-IDF Score')\n",
        "        plt.tight_layout()\n",
        "        fn = os.path.join(save_dir, f'ctfidf_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", title_prefix)}_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", cls)}.png')\n",
        "        plt.savefig(fn, dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def export_ctfidf_tables(ctfidf_dict, title_prefix, out_dir):\n",
        "    \"\"\"\n",
        "    将 c-TF-IDF 结果导出为 CSV（每个类别一个表）及总汇总 CSV。\n",
        "    \"\"\"\n",
        "    save_dir = os.path.join(out_dir, 'ctfidf')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    all_rows = []\n",
        "    for cls, df_cls in ctfidf_dict.items():\n",
        "        if df_cls.empty:\n",
        "            continue\n",
        "        df_path = os.path.join(save_dir, f'ctfidf_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", title_prefix)}_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", cls)}.csv')\n",
        "        df_cls.to_csv(df_path, index=False, encoding='utf-8')\n",
        "        for _, r in df_cls.iterrows():\n",
        "            all_rows.append({'class': cls, 'term': r['term'], 'score': r['score']})\n",
        "    if all_rows:\n",
        "        pd.DataFrame(all_rows).to_csv(os.path.join(save_dir, f'ctfidf_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", title_prefix)}_ALL.csv'),\n",
        "                                      index=False, encoding='utf-8')\n",
        "\n",
        "\n",
        "# 运行 c-TF-IDF（宏/微 与 8 类）\n",
        "if 'df' in globals() and 'OUTPUT_DIR' in globals():\n",
        "    # 宏/微：primary_label_str\n",
        "    domain_sw = [\n",
        "        'policy','policies','measure','measures','action','actions','law','laws','government','ministry','council',\n",
        "        'support','development','research','innovation','technology','science','program','programs','programme',\n",
        "        'national','international','regional','local','state','country','countries'\n",
        "    ]\n",
        "    ctfidf_primary = compute_c_tf_idf_per_class(df['text'].tolist(),\n",
        "                                                df['primary_label_str'].tolist(),\n",
        "                                                top_n=25,\n",
        "                                                stop_words_extra=domain_sw)\n",
        "    plot_ctfidf_bars(ctfidf_primary, 'c-TF-IDF (Primary Macro/Micro)', OUTPUT_DIR)\n",
        "    export_ctfidf_tables(ctfidf_primary, 'c-TF-IDF (Primary Macro/Micro)', OUTPUT_DIR)\n",
        "\n",
        "    # 8 类：ClassificationLabel\n",
        "    ctfidf_secondary = compute_c_tf_idf_per_class(df['text'].tolist(),\n",
        "                                                  df['ClassificationLabel'].tolist(),\n",
        "                                                  top_n=25,\n",
        "                                                  stop_words_extra=domain_sw)\n",
        "    plot_ctfidf_bars(ctfidf_secondary, 'c-TF-IDF (Secondary 8 Classes)', OUTPUT_DIR)\n",
        "    export_ctfidf_tables(ctfidf_secondary, 'c-TF-IDF (Secondary 8 Classes)', OUTPUT_DIR)\n",
        "    print('c-TF-IDF 类词对照 已完成并导出。')\n",
        "else:\n",
        "    print('警告：未检测到 df 或 OUTPUT_DIR，跳过 c-TF-IDF 步骤。')\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 二、可靠性评估：温度标定 + 可靠性图 + ECE\n",
        "# ==========================================\n",
        "\n",
        "def softmax_np(logits):\n",
        "    \"\"\"Numpy 版 softmax\"\"\"\n",
        "    z = logits - logits.max(axis=1, keepdims=True)\n",
        "    e = np.exp(z)\n",
        "    return e / e.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def nll_from_logits(logits, y_true):\n",
        "    \"\"\"多分类负对数似然（NLL）\"\"\"\n",
        "    probs = softmax_np(logits)\n",
        "    eps = 1e-12\n",
        "    return -np.mean(np.log(probs[np.arange(len(y_true)), y_true] + eps))\n",
        "\n",
        "\n",
        "def fit_temperature_from_logits(logits_val, y_val, max_iter=500, lr=0.01):\n",
        "    \"\"\"\n",
        "    在验证集上拟合温度 T（logits/T），最小化 NLL。\n",
        "    logits_val: [N, C] 未标定的 logit\n",
        "    返回：float T\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    L = torch.tensor(logits_val, dtype=torch.float32, device=device)\n",
        "    y = torch.tensor(y_val, dtype=torch.long, device=device)\n",
        "\n",
        "    T = torch.nn.Parameter(torch.ones(1, device=device))\n",
        "    opt = torch.optim.LBFGS([T], lr=lr, max_iter=50)\n",
        "\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def closure():\n",
        "        opt.zero_grad()\n",
        "        scaled = L / T.clamp(min=1e-3)\n",
        "        loss = loss_fn(scaled, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    for _ in range(5):\n",
        "        opt.step(closure)\n",
        "\n",
        "    return float(T.detach().cpu().item())\n",
        "\n",
        "\n",
        "def fit_temperature_from_probs(probs_val, y_val, max_iter=500, lr=0.01):\n",
        "    \"\"\"\n",
        "    针对只有概率（无logits）的模型（如 LightGBM），通过将 p 转为 logit（log p），进行温度标定：\n",
        "    new_probs = softmax( log(p) / T )\n",
        "    \"\"\"\n",
        "    eps = 1e-12\n",
        "    logits_val = np.log(np.clip(probs_val, eps, 1.0))\n",
        "    return fit_temperature_from_logits(logits_val, y_val, max_iter=max_iter, lr=lr)\n",
        "\n",
        "\n",
        "def apply_temperature_to_logits(logits, T):\n",
        "    return logits / max(T, 1e-6)\n",
        "\n",
        "\n",
        "def apply_temperature_to_probs(probs, T):\n",
        "    eps = 1e-12\n",
        "    logits = np.log(np.clip(probs, eps, 1.0))\n",
        "    scaled = apply_temperature_to_logits(logits, T)\n",
        "    return softmax_np(scaled)\n",
        "\n",
        "\n",
        "def compute_ece(probs, y_true, n_bins=15):\n",
        "    \"\"\"\n",
        "    计算 Expected Calibration Error (ECE)\n",
        "    - 以最大预测概率作为置信度\n",
        "    \"\"\"\n",
        "    confidences = probs.max(axis=1)\n",
        "    preds = probs.argmax(axis=1)\n",
        "    bins = np.linspace(0, 1, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        acc = np.mean(preds[mask] == y_true[mask])\n",
        "        conf = np.mean(confidences[mask])\n",
        "        ece += np.abs(acc - conf) * (np.sum(mask) / len(y_true))\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def plot_reliability_diagram(probs, y_true, title, out_dir, n_bins=15):\n",
        "    \"\"\"\n",
        "    绘制可靠性图（Reliability Diagram）并保存。\n",
        "    \"\"\"\n",
        "    confidences = probs.max(axis=1)\n",
        "    preds = probs.argmax(axis=1)\n",
        "    bins = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "    accs, confs = [], []\n",
        "    for i in range(n_bins):\n",
        "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
        "        if np.any(mask):\n",
        "            accs.append(np.mean(preds[mask] == y_true[mask]))\n",
        "            confs.append(np.mean(confidences[mask]))\n",
        "        else:\n",
        "            accs.append(0.0)\n",
        "            confs.append(0.0)\n",
        "\n",
        "    plt.figure(figsize=(6.5, 6.5))\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
        "    plt.plot(bin_centers, accs, 'o-', label='Accuracy per bin')\n",
        "    plt.plot(bin_centers, confs, 's--', label='Confidence per bin')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Reliability Diagram - {title}')\n",
        "    plt.legend()\n",
        "    fn = os.path.join(out_dir, f'reliability_{re.sub(r\"[^a-zA-Z0-9_]+\",\"_\", title)}.png')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fn, dpi=150)\n",
        "    plt.close()\n",
        "    return fn\n",
        "\n",
        "\n",
        "# 2.1 LightGBM 温度标定与可靠性图（Dispatcher + Macro/Micro Specialists）\n",
        "if all(v in globals() for v in ['lgbm_dispatcher', 'lgbm_macro', 'lgbm_micro', 'OUTPUT_DIR']):\n",
        "    print('开始 LightGBM 可靠性（温度标定）评估 ...')\n",
        "\n",
        "    # Dispatcher\n",
        "    P_d_val = lgbm_dispatcher.predict_proba(Xd_va)\n",
        "    P_d_test = lgbm_dispatcher.predict_proba(Xd_te)\n",
        "    T_d = fit_temperature_from_probs(P_d_val, yd_va)\n",
        "    P_d_test_cal = apply_temperature_to_probs(P_d_test, T_d)\n",
        "    ece_d_before = compute_ece(P_d_test, yd_te)\n",
        "    ece_d_after = compute_ece(P_d_test_cal, yd_te)\n",
        "    print(f'Dispatcher(LGBM) 温度 T={T_d:.3f} | ECE 前={ece_d_before:.4f} 后={ece_d_after:.4f}')\n",
        "    plot_reliability_diagram(P_d_test, yd_te, 'LGBM Dispatcher (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(P_d_test_cal, yd_te, 'LGBM Dispatcher (After TS)', OUTPUT_DIR)\n",
        "\n",
        "    # Macro Specialist\n",
        "    P_m_val = lgbm_macro.predict_proba(Xm_va)\n",
        "    P_m_test = lgbm_macro.predict_proba(Xm_te)\n",
        "    T_m = fit_temperature_from_probs(P_m_val, ym_va)\n",
        "    P_m_test_cal = apply_temperature_to_probs(P_m_test, T_m)\n",
        "    ece_m_before = compute_ece(P_m_test, ym_te)\n",
        "    ece_m_after = compute_ece(P_m_test_cal, ym_te)\n",
        "    print(f'Macro Specialist(LGBM) 温度 T={T_m:.3f} | ECE 前={ece_m_before:.4f} 后={ece_m_after:.4f}')\n",
        "    plot_reliability_diagram(P_m_test, ym_te, 'LGBM Macro Specialist (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(P_m_test_cal, ym_te, 'LGBM Macro Specialist (After TS)', OUTPUT_DIR)\n",
        "\n",
        "    # Micro Specialist\n",
        "    P_mi_val = lgbm_micro.predict_proba(xmi_va)\n",
        "    P_mi_test = lgbm_micro.predict_proba(xmi_te)\n",
        "    T_mi = fit_temperature_from_probs(P_mi_val, ymi_va)\n",
        "    P_mi_test_cal = apply_temperature_to_probs(P_mi_test, T_mi)\n",
        "    ece_mi_before = compute_ece(P_mi_test, ymi_te)\n",
        "    ece_mi_after = compute_ece(P_mi_test_cal, ymi_te)\n",
        "    print(f'Micro Specialist(LGBM) 温度 T={T_mi:.3f} | ECE 前={ece_mi_before:.4f} 后={ece_mi_after:.4f}')\n",
        "    plot_reliability_diagram(P_mi_test, ymi_te, 'LGBM Micro Specialist (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(P_mi_test_cal, ymi_te, 'LGBM Micro Specialist (After TS)', OUTPUT_DIR)\n",
        "else:\n",
        "    print('提示：未检测到 LightGBM 相关对象，跳过 LGBM 的温度标定。')\n",
        "\n",
        "\n",
        "# 2.2 DeBERTa 温度标定与可靠性图（Dispatcher + Macro/Micro Specialists）\n",
        "def build_encoded_dataset(text_series, label_series, tokenizer, max_length=512):\n",
        "    ds = Dataset.from_dict({'text': text_series.tolist(), 'label': label_series.tolist()})\n",
        "    ds = ds.map(lambda b: tokenizer(b['text'], padding='max_length', truncation=True, max_length=max_length), batched=True)\n",
        "    return ds\n",
        "\n",
        "if all(v in globals() for v in ['tr_dispatcher', 'tr_macro', 'tr_micro', 'val_df', 'test_df', 'macro_val', 'macro_test', 'micro_val', 'micro_test', 'le_primary', 'le_macro', 'le_micro', 'OUTPUT_DIR']):\n",
        "    print('开始 DeBERTa 可靠性（温度标定）评估 ...')\n",
        "\n",
        "    device = tr_dispatcher.model.device\n",
        "\n",
        "    # Dispatcher\n",
        "    tok_d = tr_dispatcher.tokenizer\n",
        "    val_ds_d = build_encoded_dataset(val_df['text'], val_df['primary_label'], tok_d)\n",
        "    test_ds_d = build_encoded_dataset(test_df['text'], test_df['primary_label'], tok_d)\n",
        "\n",
        "    logits_d_val = tr_dispatcher.predict(val_ds_d).predictions  # [N, C]\n",
        "    logits_d_test = tr_dispatcher.predict(test_ds_d).predictions\n",
        "    y_d_val = val_df['primary_label'].values\n",
        "    y_d_test = test_df['primary_label'].values\n",
        "\n",
        "    T_d_bert = fit_temperature_from_logits(logits_d_val, y_d_val)\n",
        "    probs_d_test = softmax_np(logits_d_test)\n",
        "    probs_d_test_cal = softmax_np(logits_d_test / max(T_d_bert, 1e-6))\n",
        "    ece_d_b = compute_ece(probs_d_test, y_d_test)\n",
        "    ece_d_a = compute_ece(probs_d_test_cal, y_d_test)\n",
        "    print(f'DeBERTa Dispatcher 温度 T={T_d_bert:.3f} | ECE 前={ece_d_b:.4f} 后={ece_d_a:.4f}')\n",
        "    plot_reliability_diagram(probs_d_test, y_d_test, 'DeBERTa Dispatcher (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(probs_d_test_cal, y_d_test, 'DeBERTa Dispatcher (After TS)', OUTPUT_DIR)\n",
        "\n",
        "    # Macro Specialist\n",
        "    tok_m = tr_macro.tokenizer\n",
        "    val_ds_m = build_encoded_dataset(macro_val['text'], macro_val['specialist_label'], tok_m)\n",
        "    test_ds_m = build_encoded_dataset(macro_test['text'], macro_test['specialist_label'], tok_m)\n",
        "\n",
        "    logits_m_val = tr_macro.predict(val_ds_m).predictions\n",
        "    logits_m_test = tr_macro.predict(test_ds_m).predictions\n",
        "    y_m_val = macro_val['specialist_label'].values\n",
        "    y_m_test = macro_test['specialist_label'].values\n",
        "\n",
        "    T_m_bert = fit_temperature_from_logits(logits_m_val, y_m_val)\n",
        "    probs_m_test = softmax_np(logits_m_test)\n",
        "    probs_m_test_cal = softmax_np(logits_m_test / max(T_m_bert, 1e-6))\n",
        "    ece_m_b = compute_ece(probs_m_test, y_m_test)\n",
        "    ece_m_a = compute_ece(probs_m_test_cal, y_m_test)\n",
        "    print(f'DeBERTa Macro Specialist 温度 T={T_m_bert:.3f} | ECE 前={ece_m_b:.4f} 后={ece_m_a:.4f}')\n",
        "    plot_reliability_diagram(probs_m_test, y_m_test, 'DeBERTa Macro Specialist (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(probs_m_test_cal, y_m_test, 'DeBERTa Macro Specialist (After TS)', OUTPUT_DIR)\n",
        "\n",
        "    # Micro Specialist\n",
        "    tok_mi = tr_micro.tokenizer\n",
        "    val_ds_mi = build_encoded_dataset(micro_val['text'], micro_val['specialist_label'], tok_mi)\n",
        "    test_ds_mi = build_encoded_dataset(micro_test['text'], micro_test['specialist_label'], tok_mi)\n",
        "\n",
        "    logits_mi_val = tr_micro.predict(val_ds_mi).predictions\n",
        "    logits_mi_test = tr_micro.predict(test_ds_mi).predictions\n",
        "    y_mi_val = micro_val['specialist_label'].values\n",
        "    y_mi_test = micro_test['specialist_label'].values\n",
        "\n",
        "    T_mi_bert = fit_temperature_from_logits(logits_mi_val, y_mi_val)\n",
        "    probs_mi_test = softmax_np(logits_mi_test)\n",
        "    probs_mi_test_cal = softmax_np(logits_mi_test / max(T_mi_bert, 1e-6))\n",
        "    ece_mi_b = compute_ece(probs_mi_test, y_mi_test)\n",
        "    ece_mi_a = compute_ece(probs_mi_test_cal, y_mi_test)\n",
        "    print(f'DeBERTa Micro Specialist 温度 T={T_mi_bert:.3f} | ECE 前={ece_mi_b:.4f} 后={ece_mi_a:.4f}')\n",
        "    plot_reliability_diagram(probs_mi_test, y_mi_test, 'DeBERTa Micro Specialist (Before TS)', OUTPUT_DIR)\n",
        "    plot_reliability_diagram(probs_mi_test_cal, y_mi_test, 'DeBERTa Micro Specialist (After TS)', OUTPUT_DIR)\n",
        "else:\n",
        "    print('提示：未检测到 DeBERTa 相关对象或数据集，跳过 DeBERTa 的温度标定。')\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 三、“3+5 vs 2+6” 对比图（读取 v3_1 结果）\n",
        "# ==========================================\n",
        "\n",
        "def plot_k_search_comparison(ctm_dir, out_dir):\n",
        "    \"\"\"\n",
        "    从 v3_1 的 grid_macro.json / grid_micro.json 读取每个 K 的 coherence_c_v 与 topic_diversity，\n",
        "    绘制对比曲线，并重点标注：\n",
        "      - 宏：K=3 与 K=2 的对比\n",
        "      - 微：K=5 与 K=6 的对比\n",
        "    \"\"\"\n",
        "    macro_path = os.path.join(ctm_dir, 'grid_macro.json')\n",
        "    micro_path = os.path.join(ctm_dir, 'grid_micro.json')\n",
        "\n",
        "    if not (os.path.exists(macro_path) and os.path.exists(micro_path)):\n",
        "        print('未找到 v3_1 的 K 搜索结果（grid_macro.json/grid_micro.json），跳过“3+5 vs 2+6”绘图。')\n",
        "        return\n",
        "\n",
        "    with open(macro_path, 'r', encoding='utf-8') as f:\n",
        "        macro = json.load(f)\n",
        "    with open(micro_path, 'r', encoding='utf-8') as f:\n",
        "        micro = json.load(f)\n",
        "\n",
        "    def to_df(lst):\n",
        "        return pd.DataFrame(lst).sort_values('k')\n",
        "\n",
        "    dfM = to_df(macro)\n",
        "    dfm = to_df(micro)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), sharex='col')\n",
        "    # 宏：coherence\n",
        "    axes[0,0].plot(dfM['k'], dfM['coherence_c_v'], 'o-', label='c_v')\n",
        "    axes[0,0].axvline(3, color='g', linestyle='--', label='K=3')\n",
        "    axes[0,0].axvline(2, color='r', linestyle=':', label='K=2')\n",
        "    axes[0,0].set_title('Macro: Coherence (c_v)')\n",
        "    axes[0,0].set_ylabel('c_v'); axes[0,0].legend()\n",
        "\n",
        "    # 宏：topic diversity\n",
        "    axes[1,0].plot(dfM['k'], dfM['topic_diversity'], 'o-', label='TD')\n",
        "    axes[1,0].axvline(3, color='g', linestyle='--', label='K=3')\n",
        "    axes[1,0].axvline(2, color='r', linestyle=':', label='K=2')\n",
        "    axes[1,0].set_title('Macro: Topic Diversity')\n",
        "    axes[1,0].set_xlabel('K'); axes[1,0].set_ylabel('Diversity'); axes[1,0].legend()\n",
        "\n",
        "    # 微：coherence\n",
        "    axes[0,1].plot(dfm['k'], dfm['coherence_c_v'], 'o-', label='c_v')\n",
        "    axes[0,1].axvline(5, color='g', linestyle='--', label='K=5')\n",
        "    axes[0,1].axvline(6, color='r', linestyle=':', label='K=6')\n",
        "    axes[0,1].set_title('Micro: Coherence (c_v)')\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # 微：topic diversity\n",
        "    axes[1,1].plot(dfm['k'], dfm['topic_diversity'], 'o-', label='TD')\n",
        "    axes[1,1].axvline(5, color='g', linestyle='--', label='K=5')\n",
        "    axes[1,1].axvline(6, color='r', linestyle=':', label='K=6')\n",
        "    axes[1,1].set_title('Micro: Topic Diversity')\n",
        "    axes[1,1].set_xlabel('K'); axes[1,1].legend()\n",
        "\n",
        "    plt.suptitle('v3_1 层次 CTM：3+5 vs 2+6 指标对比')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
        "    fn = os.path.join(out_dir, 'ctm_3plus5_vs_2plus6_comparison.png')\n",
        "    plt.savefig(fn, dpi=150)\n",
        "    plt.close()\n",
        "    print('已保存 “3+5 vs 2+6” 对比图：', fn)\n",
        "\n",
        "\n",
        "# 调用绘图（若 v3_1 的输出路径可用）\n",
        "if 'CTM_OUTPUT_DIR' in globals() and 'OUTPUT_DIR' in globals():\n",
        "    plot_k_search_comparison(CTM_OUTPUT_DIR, OUTPUT_DIR)\n",
        "else:\n",
        "    print('提示：未检测到 CTM_OUTPUT_DIR 或 OUTPUT_DIR，跳过 “3+5 vs 2+6” 绘图。')\n",
        "\n",
        "\n",
        "print('v3_2 增强部分（c-TF-IDF、温度标定与可靠性、3+5 vs 2+6 对比图）已执行完成。')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
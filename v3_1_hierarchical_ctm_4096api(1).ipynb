{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaLatWYQKlQ"
      },
      "source": [
        "# v3_1 层次 CTM（弱监督顶层 + 子层无监督）\n",
        "\n",
        "目标：\n",
        "- 使用 API 获取 4096 维嵌入（不降维），统一 NameEnglish + ShortDescription 作为文本；\n",
        "- 顶层：使用 ZeroShotTM（弱监督）直接区分 Macro 与 Micro（2 类），通过种子词弱监督；\n",
        "- 子层：在 Macro 子语料上选 K∈{2,3,4,5}，在 Micro 子语料上选 K∈{4,5,6,7}，用 CTM/CombinedTM 网格搜索并选择 3 与 5；\n",
        "- 防过拟合：增大 dropout、weight decay、ReduceLROnPlateau（近似早停）、较小 hidden_sizes；\n",
        "- 输出：主题词、代表文档（示例）、主题可视化（可选）、层次结构摘要，保存用于 v3_2。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DoLE0Qd0RCbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U contextualized_topic_models"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GDf-nLipXGbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 环境安装（如需）\n",
        "import sys, subprocess\n",
        "def pip_install(pkg):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
        "\n",
        "needed = [\n",
        "    'contextualized-topic-models',\n",
        "    'nltk', 'scikit-learn', 'gensim', 'openai', 'tqdm', 'pandas', 'numpy',\n",
        "    'matplotlib', 'seaborn'\n",
        "]\n",
        "for p in needed:\n",
        "    try:\n",
        "        __import__(p.split('==')[0].replace('-', '_'))\n",
        "    except Exception:\n",
        "        pip_install(p)"
      ],
      "metadata": {
        "id": "_i7TOVWPXPdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, json, time, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from openai import OpenAI\n",
        "from contextualized_topic_models.models.ctm import CombinedTM, ZeroShotTM\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# 配置路径（请根据自己的路径修改）\n",
        "INPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/验证3+5/gemini_merged_policy_data_with_labels_v2.json'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/验证3+5/v3_ctm_results'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "EMBEDDED_JSON_PATH = os.path.join(OUTPUT_DIR, 'policies_with_embeddings_api_4096.json')\n",
        "SPLIT_SUMMARY_PATH = os.path.join(OUTPUT_DIR, 'top_level_macro_micro_assignment.json')\n",
        "\n",
        "# API 配置（从环境变量读取，避免明文）\n",
        "api_key = os.environ.get('QWEN_API_KEY') or os.environ.get('CSTCLOUD_API_KEY')\n",
        "BASE_URL = os.environ.get('QWEN_BASE_URL', 'https://uni-api.cstcloud.cn/v1')\n",
        "EMBEDDING_MODEL = os.environ.get('QWEN_EMBED_MODEL', 'qwen3-embedding:8b')\n",
        "\n",
        "# 随机种子\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embed"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1) 生成 4096 维 API 嵌入，不做降维；统一使用 NameEnglish + ShortDescription\n",
        "\n",
        "def load_policies(input_path):\n",
        "    df = pd.read_json(input_path)\n",
        "    # 统一文本：NameEnglish + ShortDescription；剔除过短\n",
        "    df['text'] = (df.get('NameEnglish', '').fillna('') + '. ' + df.get('ShortDescription', '').fillna('')).astype(str)\n",
        "    df = df[df['text'].str.len() > 10].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "class Qwen3Embedding:\n",
        "    def __init__(self, api_key, base_url, model='qwen3-embedding:8b', embedding_dim=4096, batch_size=100, sleep_s=0.2):\n",
        "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
        "        self.model = model\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.sleep_s = sleep_s\n",
        "    def encode(self, texts):\n",
        "        embs = []\n",
        "        for i in tqdm(range(0, len(texts), self.batch_size), desc='Embedding via API'):\n",
        "            batch = texts[i:i+self.batch_size]\n",
        "            resp = self.client.embeddings.create(model=self.model, input=batch)\n",
        "            embs.extend([d.embedding for d in resp.data])\n",
        "            time.sleep(self.sleep_s)\n",
        "        return np.array(embs)\n",
        "\n",
        "def ensure_embeddings(df, out_path):\n",
        "    if os.path.exists(out_path):\n",
        "        print('已存在嵌入文件，直接加载:', out_path)\n",
        "        df2 = pd.read_json(out_path)\n",
        "        return df2\n",
        "    print('开始生成 4096 维嵌入...')\n",
        "    encoder = Qwen3Embedding(api_key, BASE_URL, EMBEDDING_MODEL, 4096)\n",
        "    embs = encoder.encode(df['text'].tolist())\n",
        "    df = df.copy()\n",
        "    df['embed'] = [e.tolist() for e in embs]\n",
        "    df.to_json(out_path, force_ascii=False, orient='records', indent=2)\n",
        "    print('嵌入已保存到:', out_path)\n",
        "    return df\n",
        "\n",
        "df_raw = load_policies(INPUT_JSON_PATH)\n",
        "df_emb = ensure_embeddings(df_raw, EMBEDDED_JSON_PATH)\n",
        "print('样本数:', len(df_emb))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prep"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2) 文本预处理 & 词袋\n",
        "\n",
        "def build_vectorizer(texts, min_df=5, max_df=0.9):\n",
        "    base_sw = set(stopwords.words('english'))\n",
        "    # 可按领域补充停用词（英文）\n",
        "    domain_sw = set([\n",
        "        'policy','policies','measure','measures','action','actions','law','laws','government','ministry','council',\n",
        "        'support','development','research','innovation','technology','science','program','programs','programme',\n",
        "        'country','countries','national','international','regional','local',\n",
        "        'year','years','state','states'\n",
        "    ])\n",
        "    sw = list(base_sw | domain_sw)\n",
        "    vectorizer = CountVectorizer(stop_words=sw, min_df=min_df, max_df=max_df)\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "    return vectorizer, X, vocab\n",
        "\n",
        "vectorizer_all, bow_all, vocab_all = build_vectorizer(df_emb['text'].tolist())\n",
        "bow_all = bow_all.toarray()\n",
        "emb_all = np.array(df_emb['embed'].tolist(), dtype=np.float32)\n",
        "print('BOW 维度:', bow_all.shape, 'Embed 维度:', emb_all.shape)\n",
        "\n",
        "class CTMDataset(Dataset):\n",
        "    # 简易 CTM 数据集，满足 CTM/ZeroShotTM 的输入\n",
        "    def __init__(self, X_bow, X_contextual, idx2token):\n",
        "        self.X_bow = torch.FloatTensor(X_bow)\n",
        "        self.X_contextual = torch.FloatTensor(X_contextual)\n",
        "        self.idx2token = idx2token  # 关键：用于 get_topic_lists 的词映射\n",
        "    def __len__(self):\n",
        "        return len(self.X_bow)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'X_bow': self.X_bow[idx], 'X_contextual': self.X_contextual[idx]}\n",
        "\n",
        "dataset_all = CTMDataset(bow_all, emb_all, vocab_all)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "top_level"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3) 顶层：弱监督 ZeroShotTM 划分 Macro / Micro（2 类）\n",
        "# 种子词设计（尽量概括宏观 vs 微观的语义差异）\n",
        "macro_seeds = [\n",
        "    'strategy','strategic','guideline','guidance','plan','planning','roadmap','blueprint','framework',\n",
        "    'institutional','institution','governance','coordination','layout','agenda','vision','policy_document','reform'\n",
        "]\n",
        "micro_seeds = [\n",
        "    'fund','funding','finance','subsidy','grant','tax','credit','loan','procurement','voucher','voucher',\n",
        "    'talent','education','training','scholarship','recruitment','visa','immigration',\n",
        "    'startup','incubator','accelerator','vc','venture','angel',\n",
        "    'commercialization','transfer','licensing','patent','ip','intellectual','standard','regulation','compliance'\n",
        "]\n",
        "seed_topic_list = [macro_seeds, micro_seeds]\n",
        "\n",
        "# 训练 ZeroShotTM（2 主题）。防过拟合：较大 dropout、小 hidden、ReduceLROnPlateau\n",
        "zstm = ZeroShotTM(\n",
        "    bow_size=len(vocab_all),\n",
        "    contextual_size=emb_all.shape[1],  # 4096\n",
        "    n_components=2,\n",
        "    seed_topic_list=seed_topic_list,\n",
        "    num_epochs=100,\n",
        "    hidden_sizes=(64,),              # 较小隐层\n",
        "    dropout=0.3,                     # 增强正则\n",
        "    reduce_on_plateau=True,          # 类似早停的学习率退火\n",
        "    optimizer_params={'lr': 2e-3},   # 稍低学习率\n",
        ")\n",
        "zstm.fit(dataset_all)\n",
        "\n",
        "# 文档-话题分布，获取顶层预测\n",
        "top_dist = zstm.get_doc_topic_distribution(dataset_all, n_samples=10)\n",
        "top_pred = top_dist.argmax(axis=1)\n",
        "\n",
        "# 将 0/1 对应到 Macro/Micro（根据种子主题顺序），并保存划分结果\n",
        "label_map = {0: 'Macro', 1: 'Micro'}\n",
        "df_emb_top = df_emb.copy()\n",
        "df_emb_top['TopLevel'] = [label_map[i] for i in top_pred]\n",
        "split_summary = df_emb_top['TopLevel'].value_counts().to_dict()\n",
        "json.dump({'counts': split_summary}, open(SPLIT_SUMMARY_PATH, 'w'), ensure_ascii=False, indent=2)\n",
        "print('顶层划分完成：', split_summary)\n",
        "\n",
        "# 创建 mask 和子数据集，为后续子层训练准备\n",
        "mask_macro = df_emb_top['TopLevel'] == 'Macro'\n",
        "mask_micro = df_emb_top['TopLevel'] == 'Micro'\n",
        "\n",
        "macro_df = df_emb_top[mask_macro].reset_index(drop=True)\n",
        "micro_df = df_emb_top[mask_micro].reset_index(drop=True)\n",
        "\n",
        "print(f'Macro 文档数: {len(macro_df)}, Micro 文档数: {len(micro_df)}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metrics"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 4) 子层：固定较大 K（总计 ~20）训练 CTM\n",
        "# ===============================\n",
        "# 思路：\n",
        "# - 先统计 Macro/Micro 文档占比，据此把 TOT_K 按比例分配到两个子语料。\n",
        "# - 再在各自子语料上用 CombinedTM 分别训练一次，得到较细的子层主题。\n",
        "# - 这些“细主题”用于后续人工归并到 3+5。\n",
        "\n",
        "TOT_K = 20  # 总主题数（可按需调整，比如 18/22/24）\n",
        "K_MIN_MACRO, K_MAX_MACRO = 3, 12\n",
        "K_MIN_MICRO, K_MAX_MICRO = 5, 16\n",
        "\n",
        "n_macro = int(mask_macro.sum())\n",
        "n_micro = int(mask_micro.sum())\n",
        "n_total = n_macro + n_micro\n",
        "\n",
        "# 按文档量比例分配 K\n",
        "k_macro_prop = max(K_MIN_MACRO, min(K_MAX_MACRO, int(round(TOT_K * (n_macro / max(n_total,1))))))\n",
        "k_micro_prop = TOT_K - k_macro_prop\n",
        "# 保证 micro 在范围内；若微调造成越界，则回退\n",
        "if k_micro_prop < K_MIN_MICRO:\n",
        "    k_micro_prop = K_MIN_MICRO\n",
        "    k_macro_prop = TOT_K - k_micro_prop\n",
        "if k_micro_prop > K_MAX_MICRO:\n",
        "    k_micro_prop = K_MAX_MICRO\n",
        "    k_macro_prop = TOT_K - k_micro_prop\n",
        "# 再次确保 macro 落在范围内（必要时对 TOT_K 做微调）\n",
        "if k_macro_prop < K_MIN_MACRO:\n",
        "    k_macro_prop = K_MIN_MACRO\n",
        "if k_macro_prop > K_MAX_MACRO:\n",
        "    k_macro_prop = K_MAX_MACRO\n",
        "\n",
        "print(f'将总主题数 {TOT_K} 分配为：Macro={k_macro_prop}，Micro={k_micro_prop}')\n",
        "print(f'文档数：Macro={n_macro}，Micro={n_micro}')\n",
        "\n",
        "# 为每个子语料创建独立的词袋和数据集\n",
        "macro_vec, macro_bow, macro_vocab = build_vectorizer(macro_df['text'].tolist())\n",
        "micro_vec, micro_bow, micro_vocab = build_vectorizer(micro_df['text'].tolist())\n",
        "\n",
        "macro_bow = macro_bow.toarray()\n",
        "micro_bow = micro_bow.toarray()\n",
        "\n",
        "macro_emb = np.array(macro_df['embed'].tolist(), dtype=np.float32)\n",
        "micro_emb = np.array(micro_df['embed'].tolist(), dtype=np.float32)\n",
        "\n",
        "macro_ds = CTMDataset(macro_bow, macro_emb, macro_vocab)\n",
        "micro_ds = CTMDataset(micro_bow, micro_emb, micro_vocab)\n",
        "\n",
        "print('子层数据集创建完成')\n",
        "print(f'Macro BOW: {macro_bow.shape}, Embed: {macro_emb.shape}')\n",
        "print(f'Micro BOW: {micro_bow.shape}, Embed: {micro_emb.shape}')\n",
        "\n",
        "def train_ctm_fixed_k(dataset, vocab, k, max_epochs=200, lr=2e-3, dropout=0.35, hidden=(64,)):\n",
        "    model = CombinedTM(\n",
        "        bow_size=len(vocab), contextual_size=dataset.X_contextual.shape[1], n_components=k,\n",
        "        num_epochs=max_epochs, hidden_sizes=hidden, dropout=dropout,\n",
        "        reduce_on_plateau=True, optimizer_params={'lr': lr}\n",
        "    )\n",
        "    model.fit(dataset)\n",
        "    return model\n",
        "\n",
        "# 训练 Macro 子层\n",
        "final_macro_model = train_ctm_fixed_k(macro_ds, macro_vocab, k_macro_prop)\n",
        "final_macro_topics = final_macro_model.get_topic_lists(topk=15)\n",
        "json.dump({'topics': final_macro_topics, 'k': int(k_macro_prop)}, open(os.path.join(OUTPUT_DIR, 'macro_topics.json'), 'w'), ensure_ascii=False, indent=2)\n",
        "\n",
        "# 训练 Micro 子层\n",
        "final_micro_model = train_ctm_fixed_k(micro_ds, micro_vocab, k_micro_prop)\n",
        "final_micro_topics = final_micro_model.get_topic_lists(topk=15)\n",
        "json.dump({'topics': final_micro_topics, 'k': int(k_micro_prop)}, open(os.path.join(OUTPUT_DIR, 'micro_topics.json'), 'w'), ensure_ascii=False, indent=2)\n",
        "\n",
        "print('固定 K 的子层 CTM 训练完成。')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "export"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 5) 导出核心产物，供 v3_2 有监督/评估使用\n",
        "\n",
        "# 顶层标签\n",
        "df_emb_top.to_json(os.path.join(OUTPUT_DIR, 'df_with_top_level.json'), force_ascii=False, orient='records', indent=2)\n",
        "\n",
        "# 为每个文档给出子层主题分配（可选）\n",
        "def assign_topics(model, dataset):\n",
        "    # 返回每个文档的主导主题 id\n",
        "    d2t = model.get_doc_topic_distribution(dataset, n_samples=5)\n",
        "    return d2t.argmax(axis=1)\n",
        "\n",
        "macro_assign = assign_topics(final_macro_model, macro_ds)\n",
        "micro_assign = assign_topics(final_micro_model, micro_ds)\n",
        "\n",
        "macro_export = macro_df.copy()\n",
        "macro_export['macro_topic_id'] = macro_assign\n",
        "micro_export = micro_df.copy()\n",
        "micro_export['micro_topic_id'] = micro_assign\n",
        "\n",
        "macro_export.to_json(os.path.join(OUTPUT_DIR, 'macro_docs_simple.json'), force_ascii=False, orient='records', indent=2)\n",
        "micro_export.to_json(os.path.join(OUTPUT_DIR, 'micro_docs_simple.json'), force_ascii=False, orient='records', indent=2)\n",
        "\n",
        "# 层次结构摘要\n",
        "hier_summary = {\n",
        "    'top_level_counts': split_summary,\n",
        "    'macro_best_k': int(k_macro_prop),\n",
        "    'micro_best_k': int(k_micro_prop)\n",
        "}\n",
        "json.dump(hier_summary, open(os.path.join(OUTPUT_DIR, 'hierarchical_summary.json'), 'w'), ensure_ascii=False, indent=2)\n",
        "print('导出完成。产物目录：', OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增单元：导出“主题目录”供人工判读 + 生成映射模板\n",
        "# 放置位置：紧接在固定 K 训练单元之后（或导出单元之前）\n",
        "\n",
        "def assign_topics_with_probs(model, dataset, topn=3):\n",
        "    \"\"\"为每篇文档分配主导主题，并保留前 topn 概率，便于人工核查\"\"\"\n",
        "    d2t = model.get_doc_topic_distribution(dataset, n_samples=5)  # 平滑采样\n",
        "    top_ids = d2t.argmax(axis=1)\n",
        "    top_probs = d2t.max(axis=1)\n",
        "    # 同时输出前 topn\n",
        "    topn_ids = np.argsort(-d2t, axis=1)[:, :topn]\n",
        "    topn_probs = np.take_along_axis(d2t, topn_ids, axis=1)\n",
        "    return top_ids, top_probs, topn_ids, topn_probs\n",
        "\n",
        "def build_topic_catalog(sub_df, model, topics, level_name, sample_per_topic=5):\n",
        "    \"\"\"构建可读的主题目录：每个主题的关键词、文档数、示例文本等\"\"\"\n",
        "    top_ids, top_probs, topn_ids, topn_probs = assign_topics_with_probs(model,\n",
        "                                                                        CTMDataset(\n",
        "                                                                            np.array([x for x in sub_df['_bow']]),\n",
        "                                                                            np.array([x for x in sub_df['_emb']]),\n",
        "                                                                            None\n",
        "                                                                        ),\n",
        "                                                                        topn=3)\n",
        "\n",
        "    # 把主导主题写回，便于后续导出\n",
        "    sub_df = sub_df.copy()\n",
        "    sub_df[f'{level_name}_topic_id'] = top_ids\n",
        "    sub_df[f'{level_name}_topic_prob'] = top_probs\n",
        "\n",
        "    # 统计每个主题的文档数\n",
        "    from collections import Counter\n",
        "    counts = Counter(top_ids)\n",
        "\n",
        "    catalog = []\n",
        "    for tid, words in enumerate(topics):\n",
        "        # 示例文档（按该主题的主导概率排序取前 sample_per_topic 个）\n",
        "        idxs = np.where(top_ids == tid)[0]\n",
        "        if len(idxs) > 0:\n",
        "            order = np.argsort(-top_probs[idxs])\n",
        "            show = idxs[order][:sample_per_topic]\n",
        "            samples = []\n",
        "            for i in show:\n",
        "                samples.append({\n",
        "                    'NameEnglish': sub_df.iloc[i].get('NameEnglish', ''),\n",
        "                    'ShortDescription': sub_df.iloc[i].get('ShortDescription', ''),\n",
        "                    'score': float(top_probs[i])\n",
        "                })\n",
        "        else:\n",
        "            samples = []\n",
        "        catalog.append({\n",
        "            'topic_id': int(tid),\n",
        "            'doc_count': int(counts.get(tid, 0)),\n",
        "            'keywords': words,\n",
        "            'samples': samples\n",
        "        })\n",
        "    return sub_df, catalog\n",
        "\n",
        "# 为构建 catalog 准备子语料里的 BOW/Emb（避免重复计算）\n",
        "def augment_with_bow_emb(sub_df, vec):\n",
        "    Xbow = vec.transform(sub_df['text']).toarray()\n",
        "    Xemb = np.array(sub_df['embed'].tolist(), dtype=np.float32)\n",
        "    sub_df = sub_df.copy()\n",
        "    sub_df['_bow'] = list(Xbow)\n",
        "    sub_df['_emb'] = list(Xemb)\n",
        "    return sub_df\n",
        "\n",
        "macro_df_aug = augment_with_bow_emb(macro_df, macro_vec)\n",
        "micro_df_aug = augment_with_bow_emb(micro_df, micro_vec)\n",
        "\n",
        "macro_df_cat, macro_catalog = build_topic_catalog(macro_df_aug, final_macro_model, final_macro_topics, 'macro')\n",
        "micro_df_cat, micro_catalog = build_topic_catalog(micro_df_aug, final_micro_model, final_micro_topics, 'micro')\n",
        "\n",
        "# 导出主题目录与文档-主题分配\n",
        "json.dump({'level':'macro','k': int(len(macro_catalog)),'catalog': macro_catalog}, open(os.path.join(OUTPUT_DIR,'macro_topic_catalog.json'),'w'), ensure_ascii=False, indent=2)\n",
        "json.dump({'level':'micro','k': int(len(micro_catalog)),'catalog': micro_catalog}, open(os.path.join(OUTPUT_DIR,'micro_topic_catalog.json'),'w'), ensure_ascii=False, indent=2)\n",
        "\n",
        "macro_df_cat.drop(columns=['_bow','_emb']).to_json(os.path.join(OUTPUT_DIR,'macro_docs_with_topics.json'), orient='records', force_ascii=False, indent=2)\n",
        "micro_df_cat.drop(columns=['_bow','_emb']).to_json(os.path.join(OUTPUT_DIR,'micro_docs_with_topics.json'), orient='records', force_ascii=False, indent=2)\n",
        "\n",
        "# 生成人工映射模板（请人工填写 target_label 字段为 8 类中的一个）\n",
        "EIGHT_CLASSES = [\n",
        "    'Guideline_Strategy','Planning_Layout','Institutional_Arrangements',\n",
        "    'Resource_Allocation_Policy','Innovation_Actor_Policy','Talent_Policy',\n",
        "    'Commercialization_Policy','Environment_Shaping_Policy'\n",
        "]\n",
        "\n",
        "mapping_template = {\n",
        "    'note': '请将每个子层的细主题映射到上述 8 类之一；未确定可暂留空字符串 \"\"',\n",
        "    'allowed_target_labels': EIGHT_CLASSES,\n",
        "    'macro': [{'topic_id': int(x['topic_id']), 'suggested_keywords': x['keywords'][:10], 'target_label': ''} for x in macro_catalog],\n",
        "    'micro': [{'topic_id': int(x['topic_id']), 'suggested_keywords': x['keywords'][:10], 'target_label': ''} for x in micro_catalog]\n",
        "}\n",
        "json.dump(mapping_template, open(os.path.join(OUTPUT_DIR,'topic_mapping_template.json'),'w'), ensure_ascii=False, indent=2)\n",
        "print('已导出：macro/micro 主题目录、文档-主题分配，以及 topic_mapping_template.json（请人工填写）。')"
      ],
      "metadata": {
        "id": "R_j9ML00WNNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 新增单元：应用人工映射，将无监督细主题归并为 3+5，并做覆盖/一致性检查\n",
        "# 放置位置：紧随“生成模板”单元之后\n",
        "\n",
        "MAPPING_PATH = os.path.join(OUTPUT_DIR,'topic_mapping_template.json')  # 人工填写后的路径（可改名）\n",
        "\n",
        "def apply_mapping_and_export(mapping_path):\n",
        "    mapping = json.load(open(mapping_path,'r',encoding='utf-8'))\n",
        "    allowed = set(mapping.get('allowed_target_labels', []))\n",
        "\n",
        "    def build_map_dict(items):\n",
        "        d = {}\n",
        "        for it in items:\n",
        "            tid = int(it['topic_id'])\n",
        "            tgt = it.get('target_label','').strip()\n",
        "            d[tid] = tgt\n",
        "        return d\n",
        "\n",
        "    macro_map = build_map_dict(mapping.get('macro',[]))\n",
        "    micro_map = build_map_dict(mapping.get('micro',[]))\n",
        "\n",
        "    # 载入文档-主题分配\n",
        "    macro_docs = pd.read_json(os.path.join(OUTPUT_DIR,'macro_docs_with_topics.json'))\n",
        "    micro_docs = pd.read_json(os.path.join(OUTPUT_DIR,'micro_docs_with_topics.json'))\n",
        "\n",
        "    # 应用映射\n",
        "    def map_row(row, level):\n",
        "        tid = int(row[f'{level}_topic_id'])\n",
        "        label = (macro_map if level=='macro' else micro_map).get(tid,'')\n",
        "        return label\n",
        "\n",
        "    macro_docs['MappedLabel'] = macro_docs.apply(lambda r: map_row(r,'macro'), axis=1)\n",
        "    micro_docs['MappedLabel'] = micro_docs.apply(lambda r: map_row(r,'micro'), axis=1)\n",
        "\n",
        "    # 覆盖情况\n",
        "    macro_unmapped = (macro_docs['MappedLabel']=='').sum()\n",
        "    micro_unmapped = (micro_docs['MappedLabel']=='').sum()\n",
        "    print(f'Macro 未映射文档数：{macro_unmapped} / {len(macro_docs)}')\n",
        "    print(f'Micro 未映射文档数：{micro_unmapped} / {len(micro_docs)}')\n",
        "\n",
        "    # 合并\n",
        "    merged = pd.concat([macro_docs, micro_docs], ignore_index=True)\n",
        "    # 如果原始数据带有真值标签（ClassificationLabel），可以对齐评估一致性\n",
        "    has_gt = 'ClassificationLabel' in merged.columns and merged['ClassificationLabel'].notna().any()\n",
        "    if has_gt:\n",
        "        from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "        valid = merged[merged['MappedLabel'].isin(allowed)]\n",
        "        print(f'用于一致性检查的有效样本：{len(valid)}')\n",
        "\n",
        "        y_true = valid['ClassificationLabel'].values\n",
        "        y_pred = valid['MappedLabel'].values\n",
        "\n",
        "        print('\\n[无监督主题映射 vs 真值] Classification Report:')\n",
        "        print(classification_report(y_true, y_pred, labels=sorted(allowed), zero_division=0))\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=sorted(allowed))\n",
        "        plt.figure(figsize=(10,8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(allowed), yticklabels=sorted(allowed))\n",
        "        plt.title('Unsupervised Topics (mapped) vs Ground Truth')\n",
        "        plt.xlabel('Predicted (mapped)'); plt.ylabel('True')\n",
        "        fn = os.path.join(OUTPUT_DIR, 'cm_unsup_mapped_vs_gt.png')\n",
        "        plt.tight_layout(); plt.savefig(fn, dpi=150); plt.close()\n",
        "        print('一致性混淆矩阵已保存：', fn)\n",
        "\n",
        "    # 导出最终带映射标签的数据（可供后续监督训练或分析使用）\n",
        "    merged.to_json(os.path.join(OUTPUT_DIR,'docs_with_unsup_mapped_label.json'), orient='records', force_ascii=False, indent=2)\n",
        "    print('已导出：docs_with_unsup_mapped_label.json')\n",
        "\n",
        "apply_mapping_and_export(MAPPING_PATH)"
      ],
      "metadata": {
        "id": "YYLrFAJbWOXW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
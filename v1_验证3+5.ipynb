{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpjxLG9DpP78",
        "outputId": "b738e449-eb08-465e-cf1f-13ebbe4f1473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1政策文本嵌入"
      ],
      "metadata": {
        "id": "z8l7WycPlvMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def generate_embeddings(input_file_path, output_file_path):\n",
        "    \"\"\"\n",
        "    加载政策数据，过滤无效条目，生成文本嵌入，并将结果保存到新的JSON文件。\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): 输入的JSON文件路径。\n",
        "        output_file_path (str): 输出的JSON文件路径。\n",
        "    \"\"\"\n",
        "    # 1. 加载一个强大的预训练模型\n",
        "    # all-MiniLM-L6-v2 是一个性能卓越且速度快的通用模型，\n",
        "    # 它将文本编码为 384 维的密集向量，非常适合后续的聚类和分类任务。\n",
        "    print(\"Loading Sentence-BERT model: all-MiniLM-L6-v2...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embedding_dim = model.get_sentence_embedding_dimension()\n",
        "    print(f\"Model loaded successfully. Embedding dimension: {embedding_dim}\")\n",
        "\n",
        "    # 2. 读取原始JSON数据\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            all_policies = json.load(f)\n",
        "        print(f\"Successfully loaded {len(all_policies)} total records from {input_file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file not found at {input_file_path}\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {input_file_path}\")\n",
        "        return\n",
        "\n",
        "    # 3. 过滤数据并准备用于嵌入的文本\n",
        "    valid_policies = []\n",
        "    texts_to_embed = []\n",
        "    print(\"Filtering policies and preparing text for embedding...\")\n",
        "    for policy in all_policies:\n",
        "        short_desc = policy.get('ShortDescription')\n",
        "        # 过滤机制：确保ShortDescription不为空或仅包含空白字符\n",
        "        if short_desc and str(short_desc).strip():\n",
        "            # 将原始名称和简短描述合并为一个文本字符串\n",
        "            # 如果NameOriginalLanguage为空，则使用空字符串代替\n",
        "            name_orig = policy.get('NameOriginalLanguage', '') or ''\n",
        "            combined_text = f\"{name_orig}. {short_desc}\"\n",
        "\n",
        "            texts_to_embed.append(combined_text)\n",
        "            valid_policies.append(policy)\n",
        "\n",
        "    print(f\"Found {len(valid_policies)} valid policies to process.\")\n",
        "    if not valid_policies:\n",
        "        print(\"No valid policies to process. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 4. 生成嵌入向量 (使用tqdm显示进度条)\n",
        "    print(\"Generating embeddings for all valid policies. This may take a while...\")\n",
        "    embeddings = model.encode(\n",
        "        texts_to_embed,\n",
        "        show_progress_bar=True,\n",
        "        batch_size=32  # 可以根据您的硬件调整批处理大小\n",
        "    )\n",
        "\n",
        "    # 5. 将嵌入向量添加回字典列表\n",
        "    # JSON不支持Numpy数组，因此需要将其转换为Python列表\n",
        "    for i, policy in enumerate(valid_policies):\n",
        "        policy['embed'] = embeddings[i].tolist()\n",
        "\n",
        "    # 6. 保存带有嵌入向量的新JSON文件\n",
        "    print(f\"\\nSaving data with embeddings to {output_file_path}...\")\n",
        "    try:\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(valid_policies, f, ensure_ascii=False, indent=4)\n",
        "        print(\"✅ Success!\")\n",
        "        print(f\"Processed {len(valid_policies)} policies and saved to {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file: {e}\")\n",
        "\n",
        "# --- 主程序入口 ---\n",
        "if __name__ == '__main__':\n",
        "    # 请将此路径替换为您自己的文件路径\n",
        "    INPUT_JSON_PATH = '/mnt/c/Users/20452/OneDrive/桌面/待办文件夹/【代码】/验证3+5/数据/全量标注数据/gemini_merged_policy_data_with_labels_v2.json'\n",
        "    OUTPUT_JSON_PATH = '/mnt/c/Users/20452/OneDrive/桌面/待办文件夹/【代码】/验证3+5/1政策文本嵌入/policies_with_embeddings.json'\n",
        "\n",
        "    generate_embeddings(INPUT_JSON_PATH, OUTPUT_JSON_PATH)"
      ],
      "metadata": {
        "id": "KvteNDMWmBgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KviyNCpVXfOq"
      },
      "source": [
        "# 2无监督分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8qle-c4wC6e"
      },
      "source": [
        "## ctm模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IxPKKvzv3oh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "\n",
        "# CTM的核心模型\n",
        "from contextualized_topic_models.models.ctm import CombinedTM\n",
        "\n",
        "# 使用 SKLEARN 来进行稳定可靠的文本预处理\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 使用 NLTK 来获取基础停用词列表\n",
        "import nltk\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "\n",
        "# 用于语言检测\n",
        "try:\n",
        "    from langdetect import detect\n",
        "    from langdetect.lang_detect_exception import LangDetectException\n",
        "except ImportError:\n",
        "    print(\"Warning: langdetect not installed. Installing now...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'langdetect'])\n",
        "    from langdetect import detect\n",
        "    from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "def is_english_word(word):\n",
        "    \"\"\"\n",
        "    检测单词是否为英语\n",
        "    使用多种方法来确保准确性：\n",
        "    1. 字符检测 - 只包含英文字母\n",
        "    2. 语言检测 - 使用langdetect库\n",
        "    3. 常见非英语词汇黑名单\n",
        "    \"\"\"\n",
        "    # 首先检查是否只包含英文字母（排除特殊字符和数字）\n",
        "    if not re.match(r'^[a-zA-Z]+$', word):\n",
        "        return False\n",
        "\n",
        "    # 太短的词不进行语言检测（通常是英语）\n",
        "    if len(word) <= 2:\n",
        "        return True\n",
        "\n",
        "    # 常见非英语词汇黑名单（从原始停用词列表中提取）\n",
        "    non_english_blacklist = {\n",
        "        'российской', 'visokošolskih', 'ministerijos', 'respublikos', 'technologijų',\n",
        "        'trkiye', 'zukunft', 'innovacin', 'nacional', 'bilim', 'und', 'der', 'dla',\n",
        "        'ciencia', 'tecnologa', 'tbtak', 'tubitak', 'destekleme', 'arge', 'destek',\n",
        "        'fondo', 'fonds', 'inteligencia', 'digitale', 'sanayi', 'teknoloji',\n",
        "        'operacyjny', 'pianoo', 'cooperao', 'entre', 'του', 'desenvolvimento',\n",
        "        'pesquisa', 'cientfica', 'tecnolgico', 'vlaanderen', 'vlaams', 'voor',\n",
        "        'piano', 'nazionale', 'burs', 'doktora', 'yurt', 'rencileri', 'salud',\n",
        "        'investigacin', 'ayudas', 'contratos', 'formacin', 'doctorado', 'programa',\n",
        "        'nivel', 'alto', 'datos', 'empresarial', 'centro', 'industria', 'industrie',\n",
        "        'estratégia', 'tecnologia', 'inovação', 'proyectos', 'ciência', 'innovación',\n",
        "        'tecnología', 'lietuvos', 'на', 'förderung', 'zur', 'für', 'du', 'des',\n",
        "        'την', 'και', 'για', 'türkiye', 'tübi', 'kosgeb',\n",
        "    }\n",
        "\n",
        "    if word.lower() in non_english_blacklist:\n",
        "        return False\n",
        "\n",
        "    # 使用langdetect进行语言检测\n",
        "    try:\n",
        "        detected_lang = detect(word)\n",
        "        return detected_lang == 'en'\n",
        "    except (LangDetectException, Exception):\n",
        "        # 如果检测失败，采用保守策略：\n",
        "        # 如果包含非ASCII字符，很可能不是英语\n",
        "        return all(ord(c) < 128 for c in word)\n",
        "\n",
        "class CTMDataset(Dataset):\n",
        "    \"\"\"CTM数据集类，用于创建CTM期望的数据格式\"\"\"\n",
        "\n",
        "    def __init__(self, X_bow, X_contextual):\n",
        "        self.X_bow = torch.FloatTensor(X_bow)\n",
        "        self.X_contextual = torch.FloatTensor(X_contextual)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_bow)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {\n",
        "            'X_bow': self.X_bow[index],\n",
        "            'X_contextual': self.X_contextual[index]\n",
        "        }\n",
        "\n",
        "def run_ctm_with_sklearn(input_file_path, output_file_path, num_topics=20):\n",
        "    \"\"\"\n",
        "    (V6 - FINAL - Sklearn Preprocessing)\n",
        "    使用 Scikit-learn 进行文本预处理，确保兼容性和稳定性。\n",
        "    \"\"\"\n",
        "    # --- 1. 数据加载 (不变) ---\n",
        "    print(f\"Loading data and pre-computed embeddings from {input_file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_json(input_file_path)\n",
        "        print(f\"Loaded {len(df)} documents\")\n",
        "        documents = (df['NameOriginalLanguage'].fillna('') + \". \" + df['ShortDescription'].fillna('')).astype(str).tolist()\n",
        "        sbert_embeddings = np.array(df['embed'].tolist()) # 直接转为Numpy数组\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- 2. 使用 Scikit-learn进行文本预处理 ---\n",
        "    print(\"Preprocessing text using Scikit-learn's CountVectorizer...\")\n",
        "    try:\n",
        "        stop_words.words(\"english\")\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "    base_stopwords = list(stop_words.words(\"english\"))\n",
        "    domain_specific_stopwords = [\n",
        "        # 原始自定义停用词列表\n",
        "        'para', 'use', 'based', 'provide', 'new', 'including', 'public', 'sector', 'countries', 'value',\n",
        "        'mainstreaming', 'access', 'activities', 'capacities', 'change', 'opportunities', 'quality', 'level',\n",
        "        'smes', 'sme', 'ist', 'national', 'international', 'federal', 'regional', 'european', 'российской', 'visokošolskih',\n",
        "        'canada', 'canadian', 'china', 'ge', 'ar', 'tak', 'ministerijos', 'nr', 'respublikos', 'technologijų',\n",
        "        'turkish', 'polish', 'russian', 'ukraine', 'korean', 'kazakhstan', 'bulgaria', 'greece', 'cyprus',  'ist', 'bas',\n",
        "        'hellenic', 'portugal', 'turkey', 'trkiye', 'austria', 'austrian', 'thailand', 'thai', 'german', '78', '65', 'sure',\n",
        "        'germany', 'malta', 'flanders', 'flemish', 'romanian', 'japanese', 'japan', 'india', 'zealand', 'zukunft',\n",
        "        'brasil', 'la', 'innovacin', 'nacional', 've', 'bilim', 'fr', 'und', 'der', 'en', 'na', 'dla', 'vi',\n",
        "        'ciencia', 'tecnologa', 'tbtak', 'tubitak', 'destekleme', 'arge', 'destek', 'fondo', 'fonds', 'og',\n",
        "        'inteligencia', 'digitale', 'sanayi', 'teknoloji', 'operacyjny', 'pianoo', 'cooperao', 'entre', 'του',\n",
        "        'em', 'desenvolvimento', 'pesquisa', 'cientfica', 'tecnolgico', 'vlaanderen', 'vlaams', 'voor', 'costa',\n",
        "        'piano', 'nazionale', 'burs', 'doktora', 'yk', 'yurt', 'rencileri', 'salud', 'investigacin', 'ayudas',\n",
        "        'contratos', 'formacin', 'doctorado', 'programa', 'nivel', 'alto', 'datos', 'empresarial', 'centro',\n",
        "        'industria', 'industrie', 'ministry', 'agency', 'council', '2016', '2020',\n",
        "        '3a', 'also', 'czech', 'de', 'el', 'da', 'del', 'czech', 'estratégia', 'tecnologia', 'inovação',\n",
        "        'proyectos', 'ciência', 'innovación', 'tecnología', 'ir', 'di', 'lietuvos', '2010', 'three', 'на',\n",
        "        '000', 'förderung', 'zur', 'für', '19', 'le', 'du', 'des', 'et', 'br', 'την', 'και', 'για', 'türkiye',\n",
        "        'tübi', '2019', '2021', '2017', 'bas', '78', '65', 'beis', 'ups', 'ju', 'rica', 'es', 'kosgeb',\n",
        "    ]\n",
        "\n",
        "    # 过滤非英语词汇\n",
        "    print(\"Filtering non-English words from stopwords...\")\n",
        "    english_domain_stopwords = []\n",
        "    filtered_count = 0\n",
        "\n",
        "    for word in domain_specific_stopwords:\n",
        "        if is_english_word(word):\n",
        "            english_domain_stopwords.append(word)\n",
        "        else:\n",
        "            filtered_count += 1\n",
        "            print(f\"  Filtered out non-English word: '{word}'\")\n",
        "\n",
        "    print(f\"Filtered out {filtered_count} non-English words from domain stopwords\")\n",
        "    print(f\"Remaining English domain stopwords: {len(english_domain_stopwords)}\")\n",
        "\n",
        "    # 合并英语停用词\n",
        "    final_stopwords = base_stopwords + english_domain_stopwords\n",
        "\n",
        "    # 初始化CountVectorizer，这是我们新的预处理核心\n",
        "    min_df=5 #表示一个词至少要在5个文档中出现过才被考虑\n",
        "    max_df=0.8 #表示一个词如果在90%以上的文档中都出现过，则忽略它（通常是无用词）\n",
        "    vectorizer = CountVectorizer(stop_words=final_stopwords, min_df=5, max_df=0.9)\n",
        "\n",
        "    # 一步到位：创建词袋矩阵\n",
        "    bow_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # 获取词汇表\n",
        "    vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # --- 3. 创建CTM兼容的数据集 ---\n",
        "    print(\"Creating CTM-compatible dataset...\")\n",
        "\n",
        "    # 将BOW矩阵转换为稠密矩阵\n",
        "    bow_dense = bow_matrix.toarray()\n",
        "\n",
        "    # 创建CTM数据集\n",
        "    training_dataset = CTMDataset(bow_dense, sbert_embeddings)\n",
        "    # --- ↓↓↓ 在这里添加下面这行关键代码 ↓↓↓ ---\n",
        "    # 手动将词汇表附加到数据集中，以满足get_topic_lists()的需要\n",
        "    # CTM模型内部会查找名为 idx2token 的属性来映射单词\n",
        "    training_dataset.idx2token = vocab\n",
        "    # --- ↑↑↑ 在这里添加上面这行关键代码 ↑↑↑ ---\n",
        "\n",
        "    # --- 4. 初始化和训练模型 ---\n",
        "    print(f\"Initializing and training CombinedTM with {num_topics} topics...\")\n",
        "    ctm = CombinedTM(\n",
        "        bow_size=len(vocab),\n",
        "        contextual_size=sbert_embeddings.shape[1],\n",
        "        n_components=num_topics,\n",
        "        num_epochs=50\n",
        "    )\n",
        "\n",
        "    # 直接将我们创建的字典传入模型\n",
        "    ctm.fit(training_dataset)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # --- 5. 提取并保存结果 ---\n",
        "    print(\"Extracting and formatting topics...\")\n",
        "    # 获取主题词汇，传入词汇表\n",
        "    topic_keywords = ctm.get_topic_lists(15)\n",
        "\n",
        "    output_data = []\n",
        "    for i, keywords in enumerate(topic_keywords):\n",
        "        output_data.append({\n",
        "            \"topic_id\": i,\n",
        "            \"keywords\": keywords\n",
        "        })\n",
        "\n",
        "    print(f\"Saving results to {output_file_path}...\")\n",
        "    try:\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
        "        print(\"✅ Success!\")\n",
        "        print(f\"Contextualized Topic Model results saved to {output_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON file: {e}\")\n",
        "\n",
        "# --- 主程序入口 ---\n",
        "if __name__ == '__main__':\n",
        "    INPUT_JSON_PATH = '/mnt/c/Users/20452/OneDrive/桌面/待办文件夹/【代码】/验证3+5/1政策文本嵌入/policies_with_embeddings.json'\n",
        "    OUTPUT_JSON_PATH = '/mnt/c/Users/20452/OneDrive/桌面/待办文件夹/【代码】/验证3+5/2无监督分类/ctm_results_sklearn.json'\n",
        "    NUMBER_OF_TOPICS = 20\n",
        "\n",
        "    run_ctm_with_sklearn(INPUT_JSON_PATH, OUTPUT_JSON_PATH, num_topics=NUMBER_OF_TOPICS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYKGUkcAkjIG"
      },
      "source": [
        "# 3有监督分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1RlBV0dvtnu"
      },
      "source": [
        "## 下载模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAouJz3smAGP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# --- UPDATED a---\n",
        "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
        "SAVE_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/deberta-v3-base-local'\n",
        "# --- END UPDATE ---\n",
        "\n",
        "print(f\"Downloading model and tokenizer for '{MODEL_NAME}'...\")\n",
        "print(\"This is a large model and may take several minutes...\")\n",
        "\n",
        "# Download and save the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "# Download and save the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.save_pretrained(SAVE_DIRECTORY)\n",
        "\n",
        "print(f\"\\nModel and tokenizer saved successfully to '{SAVE_DIRECTORY}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo2bikJovypa"
      },
      "source": [
        "## 两个模型的训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv1CKm227OjI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# ================== 脚本最开始的位置 (修正版) ==================\n",
        "import warnings\n",
        "\n",
        "# 忽略所有来自 scikit-learn 的 FutureWarning\n",
        "# 直接使用内置的 FutureWarning 类型，无需导入\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module='sklearn')\n",
        "\n",
        "print(\"========= DEBUG: SCRIPT VERSION V5 - CORRECTED WARNING FILTER =========\")\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import lightgbm as lgb\n",
        "import matplotlib\n",
        "matplotlib.use('Agg') # Set the backend before importing pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# At the top of your file\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DebertaV2Tokenizer\n",
        "from datasets import Dataset\n",
        "# ================== 打印实际导入的库版本 ==================\n",
        "import transformers\n",
        "print(f\"========= DEBUG: IMPORTED TRANSFORMERS VERSION: {transformers.__version__} =========\")\n",
        "# =======================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight # Added for CustomTrainer\n",
        "from torch import nn # Added for CustomTrainer\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, Trainer,\n",
        "    TrainingArguments, DebertaV2Tokenizer, DataCollatorWithPadding,\n",
        "    EarlyStoppingCallback  # <-- 1. 在这里添加 EarlyStoppingCallback 的导入\n",
        ")\n",
        "# ==============================================================================\n",
        "# 0. 配置区域\n",
        "# ==============================================================================\n",
        "class Config:\n",
        "    INPUT_JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/policies_with_embeddings.json'\n",
        "    RANDOM_STATE = 42\n",
        "    TEST_SIZE = 0.1\n",
        "    VALIDATION_SIZE = 0.1\n",
        "    LGBM_PARAMS = {\n",
        "        'objective': 'multiclass',\n",
        "        'metric': ['multi_logloss'], # Note: Early stopping will use this.\n",
        "        'n_estimators': 5000,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'num_leaves':21,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 0.1,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1,\n",
        "        'n_jobs': -1,\n",
        "        'seed': RANDOM_STATE,\n",
        "        'boosting_type': 'gbdt',\n",
        "        #'is_unbalance': True, # Alternative to class_weight, try if 'balanced' isn't enough\n",
        "    }\n",
        "    TRANSFORMER_MODEL = '/content/drive/MyDrive/Colab Notebooks/deberta-v3-base-local'\n",
        "\n",
        "    TRAINING_ARGS = {\n",
        "        'output_dir': '/content/drive/MyDrive/Colab Notebooks/results',\n",
        "        'num_train_epochs': 25,\n",
        "        'learning_rate': 1e-5,\n",
        "        'per_device_train_batch_size': 4,\n",
        "        'per_device_eval_batch_size': 8,\n",
        "        'gradient_accumulation_steps': 2,\n",
        "        'warmup_ratio': 0.1,\n",
        "        'weight_decay': 0.05,\n",
        "        'logging_dir': './logs',\n",
        "        'logging_steps': 100,\n",
        "        'eval_strategy': \"epoch\",\n",
        "        'save_strategy': \"epoch\",\n",
        "        'load_best_model_at_end': True,\n",
        "        'metric_for_best_model': 'f1',\n",
        "        'greater_is_better': True,\n",
        "        'report_to': \"none\"\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# Custom Trainer for Weighted Loss (MODIFIED FOR COMPATIBILITY)\n",
        "# ==============================================================================\n",
        "class CustomTrainer(Trainer):\n",
        "    # CORRECTED: Added **kwargs to accept new arguments from the base Trainer class\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Initialize loss function\n",
        "        loss_fct = None\n",
        "\n",
        "        # Check if we can and should compute class weights\n",
        "        if self.train_dataset and hasattr(self.train_dataset, 'features') and 'label' in self.train_dataset.features:\n",
        "            train_labels = np.array(self.train_dataset['label'])\n",
        "            if len(train_labels) > 0:\n",
        "                # Ensure labels are valid for classification\n",
        "                if (np.issubdtype(train_labels.dtype, np.integer) and np.all(train_labels >= 0) and\n",
        "                    hasattr(model, 'config') and hasattr(model.config, 'num_labels')):\n",
        "\n",
        "                    num_classes = model.config.num_labels\n",
        "                    class_weights_array = compute_class_weight(\n",
        "                        class_weight='balanced',\n",
        "                        classes=np.arange(num_classes),\n",
        "                        y=train_labels\n",
        "                    )\n",
        "                    weights_tensor = torch.tensor(class_weights_array, dtype=torch.float, device=model.device)\n",
        "                    loss_fct = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "                else:\n",
        "                    loss_fct = nn.CrossEntropyLoss()\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "        else:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 数据加载与准备 (No changes)\n",
        "# ==============================================================================\n",
        "def load_and_prepare_data(config):\n",
        "    print(\"--- Step 0: Loading and Preparing Data ---\")\n",
        "    df = pd.read_json(config.INPUT_JSON_PATH)\n",
        "    df['text'] = df['NameOriginalLanguage'].fillna('') + \". \" + df['ShortDescription'].fillna('')\n",
        "    macro_cats = ['Guideline_Strategy', 'Planning_Layout', 'Institutional_Arrangements']\n",
        "    df['primary_label_str'] = df['ClassificationLabel'].apply(lambda x: 'Macro' if x in macro_cats else 'Micro')\n",
        "    le_primary = LabelEncoder()\n",
        "    le_secondary = LabelEncoder()\n",
        "    df['primary_label'] = le_primary.fit_transform(df['primary_label_str'])\n",
        "    df['secondary_label'] = le_secondary.fit_transform(df['ClassificationLabel'])\n",
        "    train_val_df, test_df = train_test_split(\n",
        "        df, test_size=config.TEST_SIZE, random_state=config.RANDOM_STATE, stratify=df['secondary_label']\n",
        "    )\n",
        "    relative_val_size = config.VALIDATION_SIZE / (1 - config.TEST_SIZE)\n",
        "    train_df, val_df = train_test_split(\n",
        "        train_val_df, test_size=relative_val_size, random_state=config.RANDOM_STATE, stratify=train_val_df['secondary_label']\n",
        "    )\n",
        "    print(f\"Data Split: Train={len(train_df)}, Validation={len(val_df)}, Test={len(test_df)}\")\n",
        "    return df, train_df, val_df, test_df, le_primary, le_secondary\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 辅助函数（评估与可视化）(No changes)\n",
        "# ==============================================================================\n",
        "def evaluate_model(y_true, y_pred, labels, title):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    print(f\"\\n--- Evaluation Results for: {title} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Macro F1-Score: {f1:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=labels, zero_division=0))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(max(8, len(labels)*0.9), max(6, len(labels)*0.7)))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    filename = f\"confusion_matrix_{title.replace(' ', '_').replace('(', '').replace(')', '')}.png\"\n",
        "    plt.savefig(filename, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Confusion matrix saved to {filename}\")\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. LightGBM 训练器 (No changes)\n",
        "# ==============================================================================\n",
        "def train_evaluate_lgbm(train_df, val_df, test_df, label_col, le, title, apply_class_weighting=False):\n",
        "    print(f\"\\n--- Training LightGBM for: {title} ---\")\n",
        "    X_train = np.array(train_df['embed'].tolist())\n",
        "    y_train = train_df[label_col]\n",
        "    X_val = np.array(val_df['embed'].tolist())\n",
        "    y_val = val_df[label_col]\n",
        "    X_test = np.array(test_df['embed'].tolist())\n",
        "    y_test = test_df[label_col]\n",
        "\n",
        "    params = Config.LGBM_PARAMS.copy()\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        params['objective'] = 'binary'\n",
        "        params['metric'] = ['binary_logloss']\n",
        "        if 'num_class' in params: del params['num_class']\n",
        "        if apply_class_weighting:\n",
        "            params['class_weight'] = 'balanced'\n",
        "            print(f\"Applied class_weight='balanced' to LGBM (binary) for {title}.\")\n",
        "    else:\n",
        "        params['objective'] = 'multiclass'\n",
        "        params['metric'] = ['multi_logloss']\n",
        "        params['num_class'] = num_classes\n",
        "        if apply_class_weighting:\n",
        "            params['class_weight'] = 'balanced'\n",
        "            print(f\"Applied class_weight='balanced' to LGBM (multiclass) for {title}.\")\n",
        "\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_val, y_val)],\n",
        "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    metrics = evaluate_model(y_test, y_pred, le.classes_, title)\n",
        "    return model, metrics\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. Transformer (DeBERTa) 训练器 (最终报错修正版)\n",
        "# ==============================================================================\n",
        "def train_evaluate_transformer(train_df, val_df, test_df, label_col, le, title, use_custom_trainer=False):\n",
        "    print(f\"\\n--- Training Transformer for: {title} ---\")\n",
        "    train_dataset = Dataset.from_pandas(train_df[['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "    val_dataset = Dataset.from_pandas(val_df[['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "    test_dataset = Dataset.from_pandas(test_df[['text', label_col]].rename(columns={label_col: 'label'}))\n",
        "\n",
        "    tokenizer = DebertaV2Tokenizer.from_pretrained(Config.TRANSFORMER_MODEL)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        Config.TRANSFORMER_MODEL, num_labels=len(le.classes_), ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        return {\n",
        "            'accuracy': accuracy_score(labels, predictions),\n",
        "            'f1': f1_score(labels, predictions, average='macro', zero_division=0),\n",
        "        }\n",
        "\n",
        "    training_args_dict = Config.TRAINING_ARGS.copy()\n",
        "    if 'early_stopping_patience' in training_args_dict:\n",
        "        del training_args_dict['early_stopping_patience']\n",
        "\n",
        "    unique_output_dir = f\"./results/{title.replace(' ', '_')}\"\n",
        "    training_args_dict['output_dir'] = unique_output_dir\n",
        "    training_args = TrainingArguments(**training_args_dict)\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "    TrainerClass = CustomTrainer if use_custom_trainer else Trainer\n",
        "\n",
        "    trainer = TrainerClass(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[early_stopping_callback]\n",
        "    )\n",
        "\n",
        "    # 【在此处添加】手动将tokenizer附加到trainer对象上\n",
        "    # 这样在 evaluate_hierarchical_pipeline 函数中就能通过 dispatcher.tokenizer 调用它了\n",
        "    trainer.tokenizer = tokenizer\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"\\nEvaluating Transformer on test set for: {title}\")\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    y_pred = np.argmax(test_predictions.predictions, axis=-1)\n",
        "    y_true = test_dataset['label']\n",
        "    metrics = evaluate_model(y_true, y_pred, le.classes_, title)\n",
        "    return trainer, metrics\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. 分层管道评估 (No changes)\n",
        "# ==============================================================================\n",
        "def evaluate_hierarchical_pipeline(dispatcher, macro_specialist, micro_specialist, test_df, le_primary, le_secondary, model_type):\n",
        "    print(f\"\\n--- Evaluating End-to-End Hierarchical Pipeline for: {model_type.upper()} ---\")\n",
        "    y_true = test_df['secondary_label']\n",
        "    y_pred_final = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and model_type != 'lightgbm' else \"cpu\")\n",
        "    if model_type != 'lightgbm':\n",
        "        dispatcher.model.to(device)\n",
        "        macro_specialist.model.to(device)\n",
        "        micro_specialist.model.to(device)\n",
        "\n",
        "    for i, row in test_df.iterrows():\n",
        "        if model_type == 'lightgbm':\n",
        "            features = np.array(row['embed']).reshape(1, -1)\n",
        "        else:\n",
        "            features = row['text']\n",
        "\n",
        "        # Stage 1: Dispatcher prediction\n",
        "        if model_type == 'lightgbm':\n",
        "            primary_pred_encoded = dispatcher.predict(features)[0]\n",
        "        else:\n",
        "            inputs = dispatcher.tokenizer(features, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = dispatcher.model(**inputs).logits\n",
        "            primary_pred_encoded = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        primary_pred_label = le_primary.inverse_transform([primary_pred_encoded])[0]\n",
        "\n",
        "        # Stage 2: Specialist prediction\n",
        "        if primary_pred_label == 'Macro':\n",
        "            specialist_trainer = macro_specialist\n",
        "            current_specialist_le = specialist_trainer.le\n",
        "        else:\n",
        "            specialist_trainer = micro_specialist\n",
        "            current_specialist_le = specialist_trainer.le\n",
        "\n",
        "        if model_type == 'lightgbm':\n",
        "            secondary_pred_encoded_specialist = specialist_trainer.predict(features)[0]\n",
        "        else:\n",
        "            inputs = specialist_trainer.tokenizer(features, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = specialist_trainer.model(**inputs).logits\n",
        "            secondary_pred_encoded_specialist = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        final_pred_label_str = current_specialist_le.inverse_transform([secondary_pred_encoded_specialist])[0]\n",
        "        final_pred_encoded_global = le_secondary.transform([final_pred_label_str])[0]\n",
        "        y_pred_final.append(final_pred_encoded_global)\n",
        "\n",
        "    evaluate_model(y_true, y_pred_final, le_secondary.classes_, f\"End-to-End Pipeline ({model_type.upper()})\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. 主执行流程 (No changes)\n",
        "# ==============================================================================\n",
        "def main():\n",
        "    print(\"Starting Supervised Classification Pipeline...\")\n",
        "    config = Config()\n",
        "    full_df, train_df, val_df, test_df, le_primary, le_secondary = load_and_prepare_data(config)\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"RUNNING LIGHTGBM PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    dispatcher_lgbm, _ = train_evaluate_lgbm(\n",
        "        train_df, val_df, test_df,\n",
        "        'primary_label', le_primary, 'LGBM Dispatcher (Macro-Micro)',\n",
        "        apply_class_weighting=False\n",
        "    )\n",
        "\n",
        "    macro_df = full_df[full_df['primary_label_str'] == 'Macro'].copy()\n",
        "    micro_df = full_df[full_df['primary_label_str'] == 'Micro'].copy()\n",
        "    le_macro = LabelEncoder().fit(macro_df['ClassificationLabel'])\n",
        "    le_micro = LabelEncoder().fit(micro_df['ClassificationLabel'])\n",
        "    macro_df['specialist_label'] = le_macro.transform(macro_df['ClassificationLabel'])\n",
        "    micro_df['specialist_label'] = le_micro.transform(micro_df['ClassificationLabel'])\n",
        "\n",
        "    macro_train_val_df, macro_test_df = train_test_split(macro_df, test_size=0.2, random_state=config.RANDOM_STATE, stratify=macro_df['specialist_label'])\n",
        "    macro_train_df, macro_val_df = train_test_split(macro_train_val_df, test_size=0.2, random_state=config.RANDOM_STATE, stratify=macro_train_val_df['specialist_label'])\n",
        "\n",
        "    micro_train_val_df, micro_test_df = train_test_split(micro_df, test_size=0.2, random_state=config.RANDOM_STATE, stratify=micro_df['specialist_label'])\n",
        "    micro_train_df, micro_val_df = train_test_split(micro_train_val_df, test_size=0.2, random_state=config.RANDOM_STATE, stratify=micro_train_val_df['specialist_label'])\n",
        "\n",
        "    macro_specialist_lgbm, _ = train_evaluate_lgbm(\n",
        "        macro_train_df, macro_val_df, macro_test_df,\n",
        "        'specialist_label', le_macro, 'LGBM Macro Specialist',\n",
        "        apply_class_weighting=True\n",
        "    )\n",
        "    micro_specialist_lgbm, _ = train_evaluate_lgbm(\n",
        "        micro_train_df, micro_val_df, micro_test_df,\n",
        "        'specialist_label', le_micro, 'LGBM Micro Specialist',\n",
        "        apply_class_weighting=True\n",
        "    )\n",
        "    macro_specialist_lgbm.le = le_macro\n",
        "    micro_specialist_lgbm.le = le_micro\n",
        "    evaluate_hierarchical_pipeline(dispatcher_lgbm, macro_specialist_lgbm, micro_specialist_lgbm, test_df, le_primary, le_secondary, 'lightgbm')\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"RUNNING TRANSFORMER (DEBERTA) PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Using Transformer Model: {Config.TRANSFORMER_MODEL}\")\n",
        "    print(\"NOTE: This part is computationally expensive and requires a GPU with significant VRAM.\")\n",
        "    if torch.cuda.is_available():\n",
        "        dispatcher_deberta, _ = train_evaluate_transformer(\n",
        "            train_df, val_df, test_df, 'primary_label', le_primary, 'DeBERTa Dispatcher (Macro-Micro)', use_custom_trainer=False\n",
        "        )\n",
        "        macro_specialist_deberta, _ = train_evaluate_transformer(\n",
        "            macro_train_df, macro_val_df, macro_test_df, 'specialist_label', le_macro, 'DeBERTa Macro Specialist', use_custom_trainer=True\n",
        "        )\n",
        "        micro_specialist_deberta, _ = train_evaluate_transformer(\n",
        "            micro_train_df, micro_val_df, micro_test_df, 'specialist_label', le_micro, 'DeBERTa Micro Specialist', use_custom_trainer=True\n",
        "        )\n",
        "        macro_specialist_deberta.le = le_macro\n",
        "        micro_specialist_deberta.le = le_micro\n",
        "        evaluate_hierarchical_pipeline(dispatcher_deberta, macro_specialist_deberta, micro_specialist_deberta, test_df, le_primary, le_secondary, 'transformer')\n",
        "    else:\n",
        "        print(\"\\nSkipping Transformer pipeline as no GPU was detected.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}